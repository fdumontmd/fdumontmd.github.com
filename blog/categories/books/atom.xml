<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Books | Wakatta!]]></title>
  <link href="http://blog.wakatta.jp/blog/categories/books/atom.xml" rel="self"/>
  <link href="http://blog.wakatta.jp/"/>
  <updated>2011-12-18T19:53:18+09:00</updated>
  <id>http://blog.wakatta.jp/</id>
  <author>
    <name><![CDATA[Frédéric Dumont]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Seven Databases in Seven Weeks Riak on EC2]]></title>
    <link href="http://blog.wakatta.jp/blog/2011/12/17/seven-databases-in-seven-weeks-riak-on-ec2/"/>
    <updated>2011-12-17T14:36:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2011/12/17/seven-databases-in-seven-weeks-riak-on-ec2</id>
    <content type="html"><![CDATA[<p>The third day with Riak had proposed to deploy Riak on a
<a href="http://aws.amazon.com/ec2/">EC2</a> cluster as an exercise. I could not
do it then due to poor network connectivity and lack of time (I was
traveling), but I did it since and here I explain how.</p>

<!--more-->


<p>EC2 is a service from Amazon to commission a number of virtual
computers with specific performance characteristics, and with use
charged by the hour (the rate depending of the performance).</p>

<p>The whole process is fairly simple and flexible. I got my computers up
and running in minutes. Setting up Riak was a bit more involved (I
should probably had tried on my local network first), but eventually I
was able to load the data and run the queries I wanted.</p>

<p>In outline, here's the process</p>

<ul>
<li>figure out the security requirements</li>
<li>think about the cluster organisation</li>
<li>create a few instances of virtual computers</li>
<li>create the security configuration</li>
<li>connect to each machine and set it up</li>
<li>open an SSL tunnel</li>
<li>...</li>
<li>Profit!</li>
</ul>


<p>And now the details.</p>

<h3>Security requirements</h3>

<p>There will be a few machines, and I need each Riak instance to speak
to each other. This means I need to keep all the required ports open.</p>

<p>As each instance is on a different machine (presumably, I could have
several instances on each machine but I want to keep things simple), I
can use the same ports for each instance. I will just have to give
them different names.</p>

<p>Basho has a helpful
<a href="http://wiki.basho.com/Network-Security-and-Firewall-Configurations.html">page</a>
on this topic. It lists the defaults ports (all of them TCP):</p>

<ul>
<li>epmd's listener: 4369</li>
<li>handoff_port listener: 8099</li>
<li>web_port: 8098</li>
<li>pb_port: 8087</li>
<li>plus a range than can be configured. As I have just a small
network, I restrict this range to 6000-6999.</li>
</ul>


<p>These are the ports I will open in my configuration settings.</p>

<h3>Cluster organisation</h3>

<p>I want a simple setup:</p>

<ul>
<li>3 machines</li>
<li>first one is the ring leader</li>
<li>first one is also my interface for client connections</li>
</ul>


<p>So I need to set 3 machines up, make the last two join the first, and
open an SSL tunnel from my local machine to the ring leader.</p>

<h3>EC2 instances</h3>

<p>First I had to sign up to <a href="http://aws.amazon.com/">AWS</a>.</p>

<p>Once this is done, I can use the console, select the EC2 tab, and
click on "Launch Instance".</p>

<p>I chose "Launch Classic Wizard" to have as much flexibility as possible:</p>

<p><img src="https://lh6.googleusercontent.com/-MKXddn82uO8/Tu23KOzMhGI/AAAAAAAAB6Y/XpOren_ANVk/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.36.26.png"></p>

<p>Then I select a basic 64bit Amazon Linux:</p>

<p><img src="https://lh3.googleusercontent.com/--F8ajmA-58c/Tu23KHI9YNI/AAAAAAAAB6U/1fCrHF_tqg0/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.36.34.png"></p>

<p>I request 3 instances of type Micro (I'm just playing; I don't really care about performance):</p>

<p><img src="https://lh4.googleusercontent.com/-w31FLsR0iC8/Tu2222Q_ZtI/AAAAAAAAB58/E-E86lTf7xg/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.36.44.png"></p>

<p>I just click through the next two screens (I do not have any specific need for such tuning):</p>

<p><img src="https://lh5.googleusercontent.com/-kRBmxXnSwRc/Tu222wmgj4I/AAAAAAAAB50/U1Rwuqv6R9o/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.37.05.png">
<img src="https://lh3.googleusercontent.com/-WiqCpx2PT70/Tu222Iz5nTI/AAAAAAAAB5s/k1ZuuVs1yrE/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.37.08.png"></p>

<p>For the key pair, I just give a name, then download the file. I will use it later to connect to my new machines:</p>

<p><img src="https://lh4.googleusercontent.com/-JEnCFjVIM54/Tu222D8yOGI/AAAAAAAAB5o/qdAleXN-iDU/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.37.28.png"></p>

<p>For the security, I first name my group, then I can use this name as the source parameter for the rules. All the rules are Custom TCP ones; I just need to give the ports (as determined above):</p>

<p><img src="https://lh3.googleusercontent.com/-dSxG8pYQLEc/Tu222KSxpgI/AAAAAAAAB5w/AtiqjF-QTts/s640/Screen%252520Shot%2525202011-12-18%252520at%25252015.41.18.png"></p>

<p>And then I can finally start my new machines.</p>

<p>On my first attempts, two of the machines did not start; I just created a couple of new machines of the same kind (64bit Amazon Linux Micro), and put them in the same security group.</p>

<p>Here are the complete security rules (I had to add some after the initial setup):</p>

<p><img src="https://lh6.googleusercontent.com/-tSUcf1TbrLQ/Tu23KB_o3wI/AAAAAAAAB6Q/2OopRB6HFeE/s640/Screen%252520Shot%2525202011-12-18%252520at%25252017.05.42.png"></p>

<p>With this, I have a small cluster of machines. Time to connect and put
them to good use.</p>

<h3>Configuring each machine</h3>

<p>From the EC2 Dashboard, I can access my instances, and get the address for each.</p>

<p>Using the private key downloaded earlier, I can open an SSH connection
to each machine. Note that the user is by default called <code>ec2-user</code>:</p>

<p><code>
ssh -i riak-private.pem ec2-user@&lt;MACHINE_NAME&gt;
</code></p>

<p>On each, I first need to install riak. I download it directly from
Basho <a href="http://downloads.basho.com/riak/CURRENT/">website</a>.</p>

<p>Each machine has some basic tools, but no Erlang compiler. To avoid
any complications, I chose and retrieved a binary distribution:</p>

<p><code>
wget http://downloads.basho.com/riak/CURRENT/riak-1.0.2-1.el6.x86_64.rpm
</code></p>

<p>Then I  installed the  downloaded package  (<code>ec2-user</code> can  use <code>sudo</code>
without having to provide a password):</p>

<p><code>
sudo rpm -Uvh riak-1.0.2-1.el6.x86_64.rpm
</code></p>

<p>There are a few error messages (or perhaps warnings), but the package
is installed and running.</p>

<p>Riak is installed but not started yet. It is important to get the
configuration right before starting it.</p>

<h4>Common configuration</h4>

<p>For each server, I give it a name that includes the local network IP
address (not the loopback IP address <code>127.0.0.1</code>). It is important,
otherwise the machines cannot talk to each others.</p>

<p>I can get it by running <code>ifconfig</code>. I look for the ethernet setup and especially the <code>inet</code> value:
this is the IP address in Amazon private network. I need to remember
the IP address for what I will call the first machine, so I copy it
somewhere (and call it here <code>&lt;IP_ADDRESS_1&gt;</code>)</p>

<p>Using <code>sudo -e /etc/riak/vm.args</code>, I can edit the name of each node. I set it to <code>riak_ec2_1@&lt;IP_ADDRESS&gt;</code>, (or <code>riak_ec2_2</code>, <code>riak_ec2_3</code>, ...).</p>

<p>Note: the editor will be <code>vi</code>.</p>

<p>I also need to change the <code>app.config</code> file:</p>

<p><code>
sudo -e /etc/riak/app.config`
</code></p>

<p>I first need to insert a range restriction configuration. I insert the
following block near the top:</p>

<p>```
{ kernel, [</p>

<pre><code>        {inet_dist_listen_min, 6000},
        {inet_dist_listen_max, 6999}
        ]},
</code></pre>

<p>```</p>

<p>For the first machine (the one named <code>riak_ec2_1</code>), I also need to
extend the <code>http</code> interface list. By default it only listen to the
loopback interface (<code>127.0.0.1</code>), but I also need it to listen to the
ethernet interface (see above). Otherwise, I will not be able to set
the SSL tunnel up.</p>

<p>So I replace the lines that read
<code>
{http, [ {"127.0.0.1", 8098 } ]},
</code>
by something like
<code>
{http, [ {"127.0.0.1", 8098 }, {"&lt;IP_ADDRESS_1&gt;", 8098} ]},
</code></p>

<p>Once this is done, I started the instances:</p>

<p><code>
sudo riak start
</code></p>

<p>I can test each instance with:
<code>
curl http://localhost:8098/stats | python -mjson.tool
</code></p>

<p>Finally, I can link all instances together by executing on the second
and third machine:</p>

<p><code>
sudo riak-admin join riak_ec2_1@&lt;IP_ADDRESS_1&gt;
</code></p>

<p>Testing again the status should show three instances in the same ring.</p>

<p>In case of error, uninstall riak, delete the <code>/var/lib/riak</code> directory,
and start again (as I did).</p>

<h3>Setting up the SSL tunnel</h3>

<p>Nothing simpler. Using the first machine name (the one I configured
for connection from the ethernet interface):</p>

<p><code>
ssh -i riak-private.pem -f ec2-user@&lt;MACHINE_NAME&gt; \
-L 8098:ec2-user@&lt;MACHINE_NAME&gt;:8098 -N
</code></p>

<p>And now I can run from my local machine</p>

<p><code>
curl http://localhost:8098/stats | python -mjson.tool
</code></p>

<p>and get the status of the Riak server as if it was local.</p>

<h3>Loading data and running queries</h3>

<p>The book suggest to use the example data from Basho's
<a href="http://wiki.basho.com/Loading-Data-and-Running-MapReduce-Queries.html">website</a>,
which is what I did. I downloaded the
<a href="http://wiki.basho.com/attachments/goog.csv"><code>goog.csv</code></a> data file and
the <a href="http://wiki.basho.com/attachments/load_data"><code>load_data</code></a>
script. I changed the port number in the latter to use 8098 instead of
8091, then I just ran it: the data is send through the SSL tunnel and
loaded in my EC2 cluster (but it is very slow).</p>

<p>Once this was done, I checked the various queries proposed on the page, and also implemented the MapReduce Challenge:</p>

<h3>MapReduce Challenge</h3>

<p>The MapReduce Challenge is to compute the days with the highest volume
of dollar traded. This first step is to compute this value, as it is
not part of the initial data.</p>

<p>I use a definition found
<a href="http://wiki.fool.com/Daily_dollar_volume">here</a>, which makes
intuitive sense: <code>Volume*(High - Low)/2</code></p>

<h4>Dollar Traded Volume by Month</h4>

<p>I need to keep both the date and the amount, so my map function will
keep both items in a data structured indexed by month:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Dollar Traded by Month  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">function</span><span class="p">(</span><span class="nx">value</span><span class="p">,</span> <span class="nx">keyData</span><span class="p">,</span> <span class="nx">arg</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">data</span> <span class="o">=</span> <span class="nx">Riak</span><span class="p">.</span><span class="nx">mapValuesJson</span><span class="p">(</span><span class="nx">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">month</span> <span class="o">=</span> <span class="nx">value</span><span class="p">.</span><span class="nx">key</span><span class="p">.</span><span class="nx">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">).</span><span class="nx">slice</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">).</span><span class="nx">join</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">);</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">pair</span> <span class="o">=</span> <span class="p">{};</span>
</span><span class='line'>  <span class="nx">pair</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nx">value</span><span class="p">.</span><span class="nx">key</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">pair</span><span class="p">[</span><span class="s1">&#39;traded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nx">data</span><span class="p">.</span><span class="nx">Volume</span> <span class="o">*</span> <span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">High</span> <span class="o">-</span> <span class="nx">data</span><span class="p">.</span><span class="nx">Low</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">obj</span> <span class="o">=</span> <span class="p">{};</span>
</span><span class='line'>  <span class="nx">obj</span><span class="p">[</span><span class="nx">month</span><span class="p">]</span> <span class="o">=</span> <span class="nx">pair</span><span class="p">;</span>
</span><span class='line'>  <span class="k">return</span> <span class="p">[</span><span class="nx">obj</span><span class="p">];</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Then when reducing, I can compare the traded amount and keep the best:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Highest by month  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">function</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">arg</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">return</span> <span class="p">[</span> <span class="nx">values</span><span class="p">.</span><span class="nx">reduce</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">acc</span><span class="p">,</span> <span class="nx">item</span><span class="p">)</span> <span class="p">{</span><span class="o">&lt;</span><span class="err">/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">pre</span><span class="o">&gt;&lt;</span><span class="nx">code</span><span class="o">&gt;</span><span class="k">for</span><span class="p">(</span><span class="kd">var</span> <span class="nx">month</span> <span class="k">in</span> <span class="nx">item</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="k">if</span><span class="p">(</span><span class="nx">acc</span><span class="p">[</span><span class="nx">month</span><span class="p">])</span> <span class="p">{</span>
</span><span class='line'>    <span class="nx">acc</span><span class="p">[</span><span class="nx">month</span><span class="p">]</span> <span class="o">=</span> <span class="nx">acc</span><span class="p">[</span><span class="nx">month</span><span class="p">][</span><span class="s1">&#39;traded&#39;</span><span class="p">]</span> <span class="o">&amp;</span><span class="nx">lt</span><span class="p">;</span> <span class="nx">item</span><span class="p">[</span><span class="nx">month</span><span class="p">][</span><span class="s1">&#39;traded&#39;</span><span class="p">]</span> <span class="o">?</span>
</span><span class='line'><span class="o">&lt;</span><span class="err">/code&gt;&lt;/pre&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">p</span><span class="o">&gt;</span>  <span class="nx">item</span><span class="p">[</span><span class="nx">month</span><span class="p">]</span> <span class="o">:</span> <span class="nx">acc</span><span class="p">[</span><span class="nx">month</span><span class="p">];</span><span class="o">&lt;</span><span class="err">/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">pre</span><span class="o">&gt;&lt;</span><span class="nx">code</span><span class="o">&gt;</span>  <span class="p">}</span> <span class="k">else</span> <span class="nx">acc</span><span class="p">[</span><span class="nx">month</span><span class="p">]</span> <span class="o">=</span> <span class="nx">item</span><span class="p">[</span><span class="nx">month</span><span class="p">];</span>
</span><span class='line'><span class="p">}</span>
</span><span class='line'><span class="k">return</span> <span class="nx">acc</span><span class="p">;</span>
</span><span class='line'><span class="o">&lt;</span><span class="err">/code&gt;&lt;/pre&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">p</span><span class="o">&gt;</span>  <span class="p">})</span>
</span><span class='line'>  <span class="p">];</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>To test the whole, I can use the following command, then copy and
paste the query, and execute with <code>Ctrl-D</code>. The command below is run on
my local machine, but thanks to the SSL tunnel, it will execute on the
cluster:</p>

<p><code>
curl -X POST http://127.0.0.1:8098/mapred \
-H "Content-Type: application/json" -d @-
</code></p>

<p>And the query:
```
{"inputs":"goog",
 "query":[{"map":{"language":"javascript",</p>

<pre><code>              "source":"function(value, keyData, arg) {
</code></pre>

<p>  var data = Riak.mapValuesJson(value)[0];
  var month = value.key.split('-').slice(0,2).join('-');
  var pair = {};
  pair['date'] = value.key;
  pair['traded'] = data.Volume * (data.High - data.Low) / 2;
  var obj = {};
  obj[month] = pair;
  return [obj];
}"}},</p>

<pre><code>     {"reduce":{"language":"javascript",
                "source":"function(values, arg) {
</code></pre>

<p>  return [ values.reduce(function(acc, item) {</p>

<pre><code>for(var month in item) {
  if(acc[month]) {
    acc[month] = acc[month]['traded'] &lt; item[month]['traded'] ?
</code></pre>

<p>  item[month] : acc[month];</p>

<pre><code>  } else acc[month] = item[month];
}
return acc;
</code></pre>

<p>  })
  ];
}",</p>

<pre><code>     "keep":true}}]}
</code></pre>

<p>```</p>

<h4>Day by Highest Dollar Traded</h4>

<p>For this one I use a similar approach: I create pairs with the date
and the amount of dollar traded, but this pair is what I return
directly (instead of returning it indexed by month).</p>

<p>Indexing by date would not work as in the reduce step I would have to
keep the result in a simple (i.e. not indexed) variable, and therefore
would forget the date. By keeping both data items (the date and the
volume traded in dollar) in the same object, I can reduce a list of
such objects to a single item and still retain all the relevant information.</p>

<p>The mapping function creates just a pair object from the data:
<div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Dollar Traded by Day  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">function</span><span class="p">(</span><span class="nx">value</span><span class="p">,</span> <span class="nx">keyData</span><span class="p">,</span> <span class="nx">arg</span><span class="p">)</span> <span class="p">{</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">data</span> <span class="o">=</span> <span class="nx">Riak</span><span class="p">.</span><span class="nx">mapValuesJson</span><span class="p">(</span><span class="nx">value</span><span class="p">)[</span><span class="mi">0</span><span class="p">];</span>
</span><span class='line'>  <span class="kd">var</span> <span class="nx">obj</span> <span class="o">=</span> <span class="p">{};</span>
</span><span class='line'>  <span class="nx">obj</span><span class="p">[</span><span class="s1">&#39;date&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nx">value</span><span class="p">.</span><span class="nx">key</span><span class="p">;</span>
</span><span class='line'>  <span class="nx">obj</span><span class="p">[</span><span class="s1">&#39;traded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nx">data</span><span class="p">.</span><span class="nx">Volume</span> <span class="o">*</span> <span class="p">(</span><span class="nx">data</span><span class="p">.</span><span class="nx">High</span> <span class="o">-</span> <span class="nx">data</span><span class="p">.</span><span class="nx">Low</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'>  <span class="k">return</span> <span class="p">[</span><span class="nx">obj</span><span class="p">];</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>The reducing function retains the best date by volume traded in dollar
for each batch. It is simpler than the previous one as the values are
simple (i.e. not indexed):</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Highest ever  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='javascript'><span class='line'><span class="kd">function</span><span class="p">(</span><span class="nx">values</span><span class="p">,</span> <span class="nx">arg</span><span class="p">){</span>
</span><span class='line'>  <span class="k">return</span> <span class="p">[</span> <span class="nx">values</span><span class="p">.</span><span class="nx">reduce</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">acc</span><span class="p">,</span> <span class="nx">item</span><span class="p">){</span><span class="o">&lt;</span><span class="err">/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">pre</span><span class="o">&gt;&lt;</span><span class="nx">code</span><span class="o">&gt;</span>         <span class="k">if</span><span class="p">(</span><span class="nx">acc</span><span class="p">)</span> <span class="p">{</span> <span class="nx">acc</span> <span class="o">=</span> <span class="p">(</span><span class="nx">acc</span><span class="p">[</span><span class="s1">&#39;traded&#39;</span><span class="p">]</span> <span class="o">&amp;</span><span class="nx">lt</span><span class="p">;</span> <span class="nx">item</span><span class="p">[</span><span class="s1">&#39;traded&#39;</span><span class="p">])</span> <span class="o">?</span> <span class="nx">item</span> <span class="o">:</span> <span class="nx">acc</span><span class="p">;</span> <span class="p">}</span>
</span><span class='line'>         <span class="k">else</span> <span class="p">{</span> <span class="nx">acc</span> <span class="o">=</span> <span class="nx">item</span><span class="p">;</span> <span class="p">}</span>
</span><span class='line'>         <span class="k">return</span> <span class="nx">acc</span><span class="p">;</span>
</span><span class='line'>        <span class="p">})</span>
</span><span class='line'>     <span class="p">];</span>
</span><span class='line'><span class="o">&lt;</span><span class="err">/code&gt;&lt;/pre&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="o">&lt;</span><span class="nx">p</span><span class="o">&gt;</span><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>And the query:
```
{"inputs":"goog",
 "query":[{"map":{"language":"javascript",</p>

<pre><code>              "source":"function(value, keyData, arg) {
</code></pre>

<p>  var data = Riak.mapValuesJson(value)[0];
  var obj = {};
  obj['date'] = value.key;
  obj['traded'] = data.Volume * (data.High - data.Low) / 2;
  return [obj];
}"}},</p>

<pre><code>     {"reduce":{"language":"javascript",
                "source":"function(values, arg){
</code></pre>

<p>  return [ values.reduce(function(acc, item){</p>

<pre><code>             if(acc) { acc = (acc['traded'] &lt; item['traded']) ? item : acc; }
             else { acc = item[date]; }
         return acc;
        })
     ];
</code></pre>

<p>}",</p>

<pre><code>     "keep":true}}]}
</code></pre>

<p>```</p>

<p>And that's it for today.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seven Databases in Seven Weeks HBase Day 3]]></title>
    <link href="http://blog.wakatta.jp/blog/2011/12/15/seven-databases-in-seven-weeks-hbase-day-3/"/>
    <updated>2011-12-15T16:27:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2011/12/15/seven-databases-in-seven-weeks-hbase-day-3</id>
    <content type="html"><![CDATA[<p>The third day with HBase is a bit short, but opens to a world of
possibilities: the Cloud.</p>

<p>This is where HBase belongs. No personal (or even that many corporate)
networks are large enough to let it perform correctly.</p>

<p>HBase depends on a large number of servers running in parallel for
its performance, and there are few other places to find that many
machines.</p>

<!--more-->


<h3>Thrift</h3>

<p>The first topic for today is <a href="http://thrift.apache.org/">Thrift</a>, a
generic remote interface to program servers (and a gift from the new
Evil Empire, Facebook).</p>

<p>It is a tool to document a binary API, and generate client stubs to
use this API. HBase supports such an API, making it possible to write
clients in a variety of languages.</p>

<p>Using Thrift on your own project (on the server side, if you have any)
would make it possible to use different languages on the client side,
depending on whichever better fits the needs (scripting languages for
glue scripts, ...)</p>

<p>When I tried the example from the book, I had to change the connection
address of the <code>thrift_example.rb</code> code from <code>localhost</code> to
<code>127.0.0.1</code>, otherwise Thrift would refuse the connection.</p>

<h3>Whirr</h3>

<p><a href="http://whirr.apache.org/">Whirr</a> is far more exciting. It is a tool
to deploy and configure some specific servers on (among others)
<a href="http://aws.amazon.com/ec2/">Amazon EC2</a>.</p>

<p>The first, and perhaps the most complex step is to open an account on
<a href="http://aws.amazon.com/">AWS</a>. It will require a phone, a credit card,
a computer, and some time. And perhaps a couple of emails if the
account opening remains stuck in "Pending verification" status.</p>

<p>Once this is done, Whirr can be used to create instances (be careful
with that: Amazon will charge at least one hour for each server even
if you take it down after a couple of minutes), download and install
specific servers (mostly from the <a href="http://hadoop.apache.org/">Hadoop</a>
family), configure them, all of this from the comfort of the command
line (which is my case is cosily close to a cup of warm coco, so it is
very comfortable indeed).</p>

<p>All you have to do is retrieve you security token from your AWS
account page, create a public/private key pair, then write a recipe
file (which describes what kind of machines and how many you need,
what to install on each, ...), and Whirr takes care of the rest. The
first two steps only have to be done once; you can deploy as many
recipes as you need.</p>

<p>The setup process takes a few minutes, then you can connect with SSH
to one of your remote servers.</p>

<p>Whirr also creates a security configuration for each recipe, opening
only the ports that are required by the servers in the recipe,
limiting source of the connections to specific servers. You can also
edit the security rules directly in the recipe if you want.</p>

<p>The ease with which this can be done is really surprising. It reminds
me of how easy it was to deploy a Rails application on
<a href="http://www.heroku.com/">Heroku</a>.</p>

<p>Now, I do not have any foreseen uses for such computing capacity, but
I can see how it could be helpful for any organisation to be able to
run occasional large data processing jobs without having to maintain a
permanent data center.</p>

<h2>Exercises</h2>

<p>There is only one exercise today: to open a Thrift connection to an
AWS deployed HBase.</p>

<p>First is to get Thrift to run on the deployed machines. The book
suggest to connect by SSH and start the instance there, but there is a
better way if you know you will need Thrift: ask Whirr to deploy it
automatically.</p>

<p>In the file below, I've added the server <code>hbase-thriftserver</code> to the
master server:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>hbase.properties (hbase.properties)</span> <a href='/downloads/code/7d7w/hbase/hbase.properties'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
</pre></td><td class='code'><pre><code class='properties'><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Licensed to the Apache Software Foundation (ASF) under one or more</span>
</span><span class='line'><span class="c"># contributor license agreements.  See the NOTICE file distributed with</span>
</span><span class='line'><span class="c"># this work for additional information regarding copyright ownership.</span>
</span><span class='line'><span class="c"># The ASF licenses this file to You under the Apache License, Version 2.0</span>
</span><span class='line'><span class="c"># (the &quot;License&quot;); you may not use this file except in compliance with</span>
</span><span class='line'><span class="c"># the License.  You may obtain a copy of the License at</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#     http://www.apache.org/licenses/LICENSE-2.0</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Unless required by applicable law or agreed to in writing, software</span>
</span><span class='line'><span class="c"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
</span><span class='line'><span class="c"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
</span><span class='line'><span class="c"># See the License for the specific language governing permissions and</span>
</span><span class='line'><span class="c"># limitations under the License.</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># HBase 0.90.x on Cloudera Hadoop Cluster on AWS EC2</span>
</span><span class='line'><span class="c"># </span>
</span><span class='line'>
</span><span class='line'><span class="c"># Read the Configuration Guide for more info:</span>
</span><span class='line'><span class="c"># http://incubator.apache.org/whirr/configuration-guide.html</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Change the cluster name here</span>
</span><span class='line'><span class="na">whirr.cluster-name</span><span class="o">=</span><span class="s">hbase-0.90</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Change the number of machines in the cluster here</span>
</span><span class='line'><span class="na">whirr.instance-templates</span><span class="o">=</span><span class="s">1 zookeeper+hadoop-namenode+hadoop-jobtracker+hbase-master+hbase-thriftserver,1 hadoop-datanode+hadoop-tasktracker+hbase-regionserver</span>
</span><span class='line'>
</span><span class='line'><span class="c"># replication level should not be higher than number of data nodes</span>
</span><span class='line'><span class="na">hbase-site.dfs.replication</span><span class="o">=</span><span class="s">2</span>
</span><span class='line'>
</span><span class='line'><span class="c"># For EC2 set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables.</span>
</span><span class='line'><span class="na">whirr.provider</span><span class="o">=</span><span class="s">aws-ec2</span>
</span><span class='line'><span class="na">whirr.identity</span><span class="o">=</span><span class="s">${env:AWS_ACCESS_KEY_ID}</span>
</span><span class='line'><span class="na">whirr.credential</span><span class="o">=</span><span class="s">${env:AWS_SECRET_ACCESS_KEY}</span>
</span><span class='line'>
</span><span class='line'><span class="c"># The size of the instance to use. See http://aws.amazon.com/ec2/instance-types/</span>
</span><span class='line'><span class="na">whirr.hardware-id</span><span class="o">=</span><span class="s">c1.xlarge</span>
</span><span class='line'><span class="c"># Ubuntu 10.04 LTS Lucid. See http://alestic.com/</span>
</span><span class='line'><span class="na">whirr.image-id</span><span class="o">=</span><span class="s">us-east-1/ami-da0cf8b3</span>
</span><span class='line'><span class="c"># If you choose a different location, make sure whirr.image-id is updated too</span>
</span><span class='line'><span class="na">whirr.location-id</span><span class="o">=</span><span class="s">us-east-1</span>
</span><span class='line'>
</span><span class='line'><span class="c"># By default use the user system SSH keys. Override them here.</span>
</span><span class='line'><span class="na">whirr.private-key-file</span><span class="o">=</span><span class="s">keys/id_rsa</span>
</span><span class='line'><span class="na">whirr.public-key-file</span><span class="o">=</span><span class="s">keys/id_rsa.pub</span>
</span><span class='line'>
</span><span class='line'><span class="c"># The HBase version to use.</span>
</span><span class='line'><span class="na">whirr.hbase.tarball.url</span><span class="o">=</span><span class="s">http://apache.cu.be/hbase/hbase-0.90.4/hbase-0.90.4.tar.gz</span>
</span><span class='line'>
</span><span class='line'><span class="c"># The Hadoop version to use. See http://hbase.apache.org/book/hadoop.html</span>
</span><span class='line'><span class="c"># The default Hadoop version used by Whirr does not fulfill the HBase 0.90.x requirements.</span>
</span><span class='line'><span class="c"># Whirr will replace the hadoop-core jar in HBase with the one from the actually installed Hadoop.</span>
</span><span class='line'><span class="c"># This example uses Cloudera&#39;s CDH3.</span>
</span><span class='line'><span class="na">whirr.hadoop.tarball.url</span><span class="o">=</span><span class="s">http://archive.cloudera.com/cdh/3/hadoop-0.20.2-cdh3u2.tar.gz</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>As for the connection to the Thrift server, the method described in
the book is to open the port 9090 to the world, and to hope to be the
only one to know about this port: a likely possibility, but who would
want to take such a chance in production?</p>

<p>Fortunately, there is a better solution: SSH Tunneling. It is very
easy to set up and requires nothing but what we already have.</p>

<p>The general idea is to open a ssh tunnel between a local port and a
remote port: whatever you puts in the local port is taken by ssh,
transported over the SSH connection; once it reaches the remote
machine, the remote ssh instance will forward the data to the remote
port, as if it was a client running on the remote machine.</p>

<p>The transport between the two machines only requires the remote one to
have the SSH port open (which is both the case, and secure). You have
to use authentication and encryption for the transport.</p>

<p>And what is required to implement this SSH tunneling:</p>

<p><code>
ssh -i keys/id_rsa -f ${USER}@&lt;SERVER_NAME&gt; -L 9090:&lt;SERVER_NAME&gt;:9090 -N
</code></p>

<p>(from the directory where you created the <code>keys</code> directory)</p>

<p>Here I map the local port 9090 to the remote machine's port 9090. That
way I don't even have to change my <code>thrift_example.rb</code> code. But of
course, if I had to connect to different machines, I would use
different ports.</p>

<p>The Thrift server was automatically started by the recipe.</p>

<p>With this in place, and after creating some tables in the remote
HBase:</p>

<p>```
$ ruby thrift_example.rb
links
  from:</p>

<pre><code>maxVersions: 1
compression: NONE
bloomFilterType: ROWCOL
</code></pre>

<p>  to:</p>

<pre><code>maxVersions: 1
compression: NONE
bloomFilterType: ROWCOL
</code></pre>

<p>wiki
  revision:</p>

<pre><code>maxVersions: 2147483647
compression: NONE
bloomFilterType: NONE
</code></pre>

<p>  text:</p>

<pre><code>maxVersions: 2147483647
compression: GZ
bloomFilterType: ROW
</code></pre>

<p>```</p>

<p>(be careful not to use LZO as a compression algorithm in the remote
HBase, as I did when I tried the first time: the default HBase has no
LZO support and will fail when you try to enable a table with LZO compression).</p>

<p>To take a tunnel down, you'll have to find and kill it (as far as I
can tell). If you have no other ssh connections, <code>killall ssh</code> is a
simple solution. In any case, the connection will be cut when the
remote servers are destroyed.</p>

<h2>Wrapping up HBase</h2>

<p>I like what I see with HBase: the project has strong backers among its
users (Yahoo, Facebook, ...); it belongs to a large family of tools
that help to design Big Data solutions, and integrates well with some
Cloud networks</p>

<p>The model is easy to understand (the book mentions the possibility of
eventual consistency due to regional replication, but this remains a
simpler model than Riak's), and close to the original MapReduce
concept.</p>

<p>This is really one tool I will have a closer look to in the near
future.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seven Databases in Seven Weeks HBase Day 2]]></title>
    <link href="http://blog.wakatta.jp/blog/2011/12/12/seven-databases-in-seven-weeks-hbase-day-2/"/>
    <updated>2011-12-12T23:46:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2011/12/12/seven-databases-in-seven-weeks-hbase-day-2</id>
    <content type="html"><![CDATA[<p>And on the second day with HBase, we load it with <a href="http://www.wikipedia.org/">Wikipedia</a>. Actually I had to do it twice to get it to work: on my first attempt the process kind of froze at about 200,000 articles.</p>

<!--more-->


<p>After some digging (and finding this very helpful <a href="http://ofps.oreilly.com/titles/9781449396107/installation.html">page</a> from <a href="http://ofps.oreilly.com/titles/9781449396107/">HBase: The Definitive Guide</a>), I tried again with a different setting for the limit on open files:</p>

<p><code>
ulimit -n 10240
</code></p>

<p>With that, HBase was able to keep churning along (the limit is per session, so HBase had to be restarted). I started the import process in the morning, and when I finally stopped it it had passed 10,000,000 pages (not all of them actual articles). Parsing links was equally successful.</p>

<h3>Consistency</h3>

<p>Unlike Riak, which offers <a href="http://en.wikipedia.org/wiki/Eventual_consistency">eventual consistency</a>, HBase ensures row level consistency. This means that each row has only one value, and a write to the row is either entirely successful, or not performed at all (so an update will never be applied partially).</p>

<p>This idea that each row is atomic is a simple yet effective mental model; I feel I should be able to use this model to design reliable solutions on HBase. To make them fast as well is a different matter entirely: I'd first need more experience with the concept of column families and their various options.</p>

<h3>Logging</h3>

<p>HBase uses <a href="http://en.wikipedia.org/wiki/Write-ahead_logging">Write-Ahead Logging</a>, exactly like PostgreSQL and many other databases (Riak too) and file systems. This is a low level mechanism designed to help with consistency: first a description of the updates is written into a log file (and flushed); then the update is performed. If there's a problem during the update, it is always possible to compare the write-ahead log and execute again whatever updates are missing or partial.</p>

<h3>Regions and servers</h3>

<p>I must say I am still a bit unclear on this topic: I have a standalone instance of HBase, so naturally there is no distribution involved.</p>

<p>HBase first keep the data sorted by key, and distributes contiguous chunks of data to each region (growing the number of regions if needed).</p>

<h3>HBase and names</h3>

<p>In a typical relational database, just as in a normal programming language, the name you give to things (tables, columns or variables) is a programmer oriented feature that has no impact on performance.</p>

<p>The idea that you should use short variable names for 'performance reason' is either a joke or a beginner's mistake.</p>

<p>Except in HBase, where the length of names can impact storage performance. See the <a href="http://hbase.apache.org/book.html#rowkey.design">HBase book, Try to minimize row and column sizes</a>.</p>

<h2>Exercises</h2>

<h3>Compression in HBase</h3>

<p>I could not really find any article on the pros and cons of compression in either HBase or Hadoop. I guess the pros and cons here are the same as any other use of compression: trading IO for CPU. Smaller (because compressed) data can be saved to and read from the disk faster, but at the cost of higher CPU usage.</p>

<h3>Bloom filters</h3>

<p>Bloom filters are describe on the always helpful <a href="http://en.wikipedia.org/wiki/Bloom_filter">Wikipedia</a>. Such a filter is a tool to determine quickly if a piece of information in not in a specific storage, with a configurable probability for false positive.</p>

<p>Say you have a key value distributed data store. For each store, you maintain a Bloom filter of the keys.</p>

<p>Assuming you are looking for a key, you can use the Bloom filters to quickly determine where to look further.</p>

<p>If a Bloom filter for a store states the key is not present, you know you can ignore the store. If it says the key is present, it could be wrong, so you have to look. How often it returns yes when it should say no is a trade-off between the size of the filter and the probability of error.</p>

<p>With HBase being distributed by default, knowing where to look for a key or a key, column pair can increase performance.</p>

<h3>Column family options for compression</h3>

<p>There use to be <code>RECORD</code> and <code>BLOCK</code> options, but they appear deprecated. What is left is to specify the compression algorithm for either regular compression, or compacting compression (which happens when HBase reorganize the store). The compacting compression setting can use the same values (i.e. algorithm names) as the compression setting. In the shell, the option is <code>COMPRESSION_COMPACT</code>.</p>

<p>The available algorithms are <code>NONE</code> (no encryption), <code>GZ</code>, <code>LZO</code> and <code>SNAPPY</code> (which is probably better still than LZO).</p>

<h3>Column family compression design consideration</h3>

<p>I could not find any definitive answer to this, but I would guess that:</p>

<ul>
<li>already compressed data (such as JPEG) should be in an uncompressed column family</li>
<li>rarely used by very large data could use a slower but more efficient algorithm such as GZ</li>
<li>small but very often used families should not be compressed</li>
</ul>


<h3>Installing LZO</h3>

<p>To install LZO compression is not exactly trivial, especially on Mac OS X.</p>

<p>The first step is to install the library; I did it with <a href="http://mxcl.github.com/homebrew/">Homebrew</a>. It installs 64 bits versions by default; the only thing to remember is that by default on Mac OS X 10.7, the default compiler is <a href="http://llvm.org">LLVM</a>, but often <a href="http://gcc.gnu.org/">GCC</a> is better.</p>

<p><code>
sudo brew install lzo --use-gcc
</code></p>

<p>and LZO will end up under <code>/usr/local/Cellar/lzo/2.06/</code></p>

<p>Next step is to build the hadoop LZO plugin. The basic information is available on the Hadoop <a href="http://wiki.apache.org/hadoop/UsingLzoCompression">wiki</a>, but the main repository it refers to is obsolete. There is another, maintained <a href="https://github.com/toddlipcon/hadoop-lzo">repository</a> on Github.</p>

<p><code>
git clone https://github.com/toddlipcon/hadoop-lzo
</code></p>

<h4>Mac OS X</h4>

<p>Building on Linux should work right away, but Mac OS X (especially 10.7) is slightly different in frustrating way. The <code>ld</code> command is not GNU, but BSD, so it does not understand the same options.</p>

<p>To get the library to compile, you need to edit the <code>build.xml</code> file and clear the <code>LDFLAGS</code> (by default the value is <code>-Wl,--no-as-needed</code>, it needs to be empty).</p>

<p>Liquid error: ClassNotFound: no lexer for alias 'txt' found</p>

<p>From inside the repository, it can be applied with</p>

<p><code>
patch -p1 &lt; hadoop-lzo.patch
</code></p>

<p>Once this is done, the <code>ant</code> invocation documented in the Wiki should almost work. Two things need to be changed: first is the use of <code>GCC</code> instead of <code>LLVM</code> (by setting the <code>CC</code> variable); second is the strange name of the <code>include</code> directory for Java. The build script expects it under <code>$JAVA_HOME/include</code>, but of course in Mac OS X it had to be somewhere else (<code>/System/Library/Frameworks/JavaVM.framework/Headers</code>, if you need to know), so I added it directly to the include path <code>C_INCLUDE_PATH</code>:</p>

<p><code>
env JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home/ \
C_INCLUDE_PATH=/System/Library/Frameworks/JavaVM.framework/Headers:/usr/local/Cellar/lzo/2.06/include/ \
LIBRARY_PATH=/usr/local/Cellar/lzo/2.06/lib/ CFAGS='-arch x86_64' \
CC=/usr/bin/gcc-4.2  ant clean compile-native test tar
</code></p>

<p>Normally, you should now have a <code>build</code> directory with the jar and native libraries.</p>

<p>The final step is to deploy this in HBase. HBase expect everything to be under the <code>$HBASE_HOME/lib</code>. The instructions from the wiki give the right commands (I just added the creation of the <code>$HBASE_HOME/lib/native</code> directory, which does not exist by default):</p>

<p><code>
cp build/hadoop-lzo-0.4.15/hadoop-lzo-0.4.15.jar $HBASE_HOME/lib/
mkdir -p $HBASE_HOME/lib/native
tar -cBf - -C build/hadoop-lzo-0.4.15/lib/native/ . | tar -xBvf - -C $HBASE_HOME/lib/native
</code></p>

<p>Now you can test whether the new library is enabled: run the command:</p>

<p><code>
$ ./bin/hbase org.apache.hadoop.hbase.util.CompressionTest /tmp/data.lzo lzo
</code></p>

<p>and it should output:</p>

<p><code>
11/12/14 09:13:21 INFO lzo.GPLNativeCodeLoader: Loaded native gpl library
11/12/14 09:13:21 INFO lzo.LzoCodec: Successfully loaded &amp; initialized native-lzo library [hadoop-lzo rev c7d54fffe5a853c437ee23413ba71fc6af23c91d]
11/12/14 09:13:21 INFO compress.CodecPool: Got brand-new compressor
SUCCESS
</code></p>

<p>And that's it. The most frustrating part is that HBase will appear to hang when you try to enable a table that uses LZO compression if anything went wrong (and forgot to test as above). The logs will reveal that <code>hadoop-native</code> cannot be found. This means that the native libraries cannot be loaded. So make sure that you have all the files below:</p>

<p><code>
$HBASE_HOME/lib/native/Mac_OS_X-x86_64-64/libgplcompression.0.dylib
$HBASE_HOME/lib/native/Mac_OS_X-x86_64-64/libgplcompression.a
$HBASE_HOME/lib/native/Mac_OS_X-x86_64-64/libgplcompression.dylib
$HBASE_HOME/lib/native/Mac_OS_X-x86_64-64/libgplcompression.la
</code></p>

<p>After that, restart the server, and you can use LZO compression instead of GZ.</p>

<p>And this completes Day 2. Next and final day is about deploying HBase to the cloud. This might take more than just a day as I need some time to figure out how to use <a href="http://aws.amazon.com/ec2/">AWS EC2</a> and which options to choose, but hopefully I'll be able to deploy Riak there as well.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seven Databases in Seven Weeks HBase Day 1]]></title>
    <link href="http://blog.wakatta.jp/blog/2011/12/11/seven-databases-in-seven-weeks-hbase-day-1/"/>
    <updated>2011-12-11T12:57:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2011/12/11/seven-databases-in-seven-weeks-hbase-day-1</id>
    <content type="html"><![CDATA[<p>New week, new database. This week is about <a href="">HBase</a>, a product that
has a significant enterprisy feel about it. First it is written in
Java, the de facto enterprise language. Then it is already in
production in very large big data consumers (Facebook among others).</p>

<p>Perhaps more surprising is the fact that it even runs at all on a
single, personal computer (as the book states, 5 dedicated servers is
the recommended minimal configuration).</p>

<!--more-->


<p>Today is a fairly short day. Getting HBase to run, creating a single
table and a couple of rows, and that's it.</p>

<p>As for Riak, I recommend downloading the
<a href="http://www.apache.org/dyn/closer.cgi?path=hbase/hbase-0.90.3/hbase-0.90.3.tar.gz">HBase package</a>
rather than trying your luck with the Homebrew version. HBase runs
directly from the extraction directory, and already includes all the
dependencies.</p>

<p>Just edit the hbase-site.xml configuratio file as the book recommends,
and you're good to go.</p>

<h2>Exercises</h2>

<h3>put_many function</h3>

<p>This function is more an exercise in Ruby than in HBase. The code is
just a variant of what is already in the book.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>put_many.rb  (put_many.rb)</span> <a href='/downloads/code/7d7w/hbase/put_many.rb'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="c1"># recap some definition to make this definition self-contained</span>
</span><span class='line'><span class="n">import</span> <span class="s1">&#39;org.apache.hadoop.hbase.client.HTable&#39;</span>
</span><span class='line'><span class="n">import</span> <span class="s1">&#39;org.apache.hadoop.hbase.client.Put&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="k">def</span> <span class="nf">jbytes</span><span class="p">(</span> <span class="o">*</span><span class="n">args</span> <span class="p">)</span>
</span><span class='line'>  <span class="n">args</span><span class="o">.</span><span class="n">map</span> <span class="p">{</span> <span class="o">|</span><span class="n">arg</span><span class="o">|</span> <span class="n">arg</span><span class="o">.</span><span class="n">to_s</span><span class="o">.</span><span class="n">to_java_bytes</span> <span class="p">}</span>
</span><span class='line'><span class="k">end</span>
</span><span class='line'>
</span><span class='line'><span class="c1"># actual exercise</span>
</span><span class='line'><span class="k">def</span> <span class="nf">put_many</span><span class="p">(</span> <span class="n">table_name</span><span class="p">,</span> <span class="n">row</span><span class="p">,</span> <span class="n">column_values</span><span class="p">)</span>
</span><span class='line'>  <span class="n">table</span> <span class="o">=</span> <span class="no">HTable</span><span class="o">.</span><span class="n">new</span><span class="p">(</span> <span class="vi">@hbase</span><span class="o">.</span><span class="n">configuration</span><span class="p">,</span> <span class="n">table_name</span> <span class="p">)</span>
</span><span class='line'>
</span><span class='line'>  <span class="nb">p</span> <span class="o">=</span> <span class="no">Put</span><span class="o">.</span><span class="n">new</span><span class="p">(</span> <span class="o">*</span><span class="n">jbytes</span><span class="p">(</span> <span class="n">row</span> <span class="p">))</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">column_values</span><span class="o">.</span><span class="n">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="o">|</span>
</span><span class='line'>    <span class="p">(</span><span class="n">kf</span><span class="p">,</span> <span class="n">kn</span><span class="p">)</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">kn</span> <span class="o">||=</span> <span class="s2">&quot;&quot;</span>
</span><span class='line'>    <span class="nb">p</span><span class="o">.</span><span class="n">add</span><span class="p">(</span> <span class="o">*</span><span class="n">jbytes</span><span class="p">(</span> <span class="n">kf</span><span class="p">,</span> <span class="n">kn</span><span class="p">,</span> <span class="n">v</span> <span class="p">))</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'>
</span><span class='line'>  <span class="n">table</span><span class="o">.</span><span class="n">put</span><span class="p">(</span> <span class="nb">p</span> <span class="p">)</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<h3>Use the put_many function</h3>

<p>Invoking the <code>put_many</code> function then checking the insert:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>Testing put_many  </span></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">put_many</span> <span class="s1">&#39;wiki&#39;</span><span class="p">,</span> <span class="s1">&#39;Some title&#39;</span><span class="p">,</span> <span class="p">{</span>
</span><span class='line'>  <span class="s2">&quot;text:&quot;</span> <span class="o">=&gt;</span> <span class="s2">&quot;Some article text&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="s2">&quot;revision:author&quot;</span> <span class="o">=&gt;</span> <span class="s2">&quot;jschmoe&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="s2">&quot;revision:comment&quot;</span> <span class="o">=&gt;</span> <span class="s2">&quot;no comment&quot;</span> <span class="p">}</span><span class="o">&lt;</span><span class="sr">/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="sr">&lt;p&gt;get &#39;wiki&#39;, &#39;Some title&#39;</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>generates</p>

<p><code>
COLUMN                CELL                                                      
 revision:author      timestamp=1323575657943, value=jschmoe                    
 revision:comment     timestamp=1323575657943, value=no comment                 
 text:                timestamp=1323575657943, value=Some article text          
3 row(s) in 0.5340 seconds
</code></p>

<p>And that's all for today. Tomorrow will be a bit more fun: first a
significant take on of Wikipedia files, then using HBase to play with
the loaded data.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Seven Databases in Seven Weeks Riak Day 3]]></title>
    <link href="http://blog.wakatta.jp/blog/2011/12/09/seven-databases-in-seven-weeks-riak-day-3/"/>
    <updated>2011-12-09T22:32:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2011/12/09/seven-databases-in-seven-weeks-riak-day-3</id>
    <content type="html"><![CDATA[<p>Today we complete the tour of Riak features. First conflict resolution
with vector clocks; then pre and post-commit hooks, and finally
searching Riak data with a <a href="http://lucene.apache.org/">Solr</a>
compatible interface.</p>

<!--more-->


<h3>Setting up Riak</h3>

<p>Over the last few days, I have been trying different ways to get Riak up and running.</p>

<p>Following the book advice, I recommend installing Riak from the
sources. Actually, you can just build it, build the <code>devrel</code> target,
and run from the <code>dev</code> directory.</p>

<p>On Mac OS X, <a href="http://mxcl.github.com/homebrew/">Homebrew</a> usually
works for me, but I like my servers to run with their own user, so I
<code>sudo brew install</code> the packages. In the case of Riak, this does not
work at all (the files have incorrect ownership and useless permissions).</p>

<p>There is a small bug in Riak 1.0.2 that causes it to return a 500 HTTP
error when the precommit hook fails, instead of the expected 403. The
problem is a spurious variable binding in Erlang. The patch below
fixes this error; it also make it possible to build Riak with Erlang R14B04.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>riak 1.0.2 patch  (riak-1.0.2.patch)</span> <a href='/downloads/code/7d7w/riak/riak-1.0.2.patch'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
</pre></td><td class='code'><pre><code class='text'><span class='line'>diff -rupN riak-1.0.2-orig/deps/riak_kv/src/riak_kv_wm_object.erl riak-1.0.2/deps/riak_kv/src/riak_kv_wm_object.erl
</span><span class='line'>--- riak-1.0.2-orig/deps/riak_kv/src/riak_kv_wm_object.erl	2011-11-18 04:50:52.000000000 +0900
</span><span class='line'>+++ riak-1.0.2/deps/riak_kv/src/riak_kv_wm_object.erl	2011-12-11 13:27:52.000000000 +0900
</span><span class='line'>@@ -966,8 +966,8 @@ handle_common_error(Reason, RD, Ctx) -&gt;
</span><span class='line'>     case {error, Reason} of
</span><span class='line'>         {error, precommit_fail} -&gt;
</span><span class='line'>             {{halt, 403}, send_precommit_error(RD, undefined), Ctx};
</span><span class='line'>-        {error, {precommit_fail, Reason}} -&gt;
</span><span class='line'>-            {{halt, 403}, send_precommit_error(RD, Reason), Ctx};
</span><span class='line'>+        {error, {precommit_fail, Message}} -&gt;
</span><span class='line'>+            {{halt, 403}, send_precommit_error(RD, Message), Ctx};
</span><span class='line'>         {error, too_many_fails} -&gt;
</span><span class='line'>             {{halt, 503}, wrq:append_to_response_body(&quot;Too Many write failures&quot;
</span><span class='line'>                     &quot; to satisfy W/DW\n&quot;, RD), Ctx};
</span><span class='line'>diff -rupN riak-1.0.2-orig/rebar.config riak-1.0.2/rebar.config
</span><span class='line'>--- riak-1.0.2-orig/rebar.config	2011-11-18 05:17:47.000000000 +0900
</span><span class='line'>+++ riak-1.0.2/rebar.config	2011-12-11 13:28:21.000000000 +0900
</span><span class='line'>@@ -1,6 +1,6 @@
</span><span class='line'> {sub_dirs, [&quot;rel&quot;]}.
</span><span class='line'>
</span><span class='line'>-{require_otp_vsn, &quot;R14B0[23]&quot;}.
</span><span class='line'>+{require_otp_vsn, &quot;R14B0[234]&quot;}.
</span><span class='line'>
</span><span class='line'> {cover_enabled, true}.
</span><span class='line'>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Apply it in the Riak 1.0.2 directory: <code>patch -p1 &lt; ../riak-1.0.2.patch</code>.</p>

<p>It can also be applied on a repository pulled from <a href="https://github.com/basho/riak">github</a>, but the
latest version has already the patch for Erlang R14B04, so you can
ignore that patch.</p>

<p><em>Update (2012/12/13):</em> my fix has been merged into the main Riak github repository,
so you no longer need to apply the patch if you get the latest source
by Git.</p>

<h3>Vector Clocks</h3>

<p><a href="http://en.wikipedia.org/wiki/Vector_clock">Vector clocks</a> are a
common mechanism to attach a precise time to events occurring
concurrently. By having each event producer keep tracks of the vector
clock of events it is responding to, it becomes possible to identify
sequences and branches in the timing of events (when running
concurrently, time is no longer linear, but can be a graph, or even
worse, a tree, where various participants ignore each other's
responses).</p>

<p>In particular, vector clocks allow a participant to detect conflicts
in the data, and take remedial actions.</p>

<p>Unfortunately, the notion of conflict resolution is not trivial; and
with Riak all or nothing updates, there is almost no information left
to do a merge (with concurrent file modifications, on the other hand,
if the updated areas are different, it might be possible to apply both
changes to the original version, which is what version control systems
typically do).</p>

<p>It does not help that, once again, the example chosen by the authors
to illustrate the concept is poor: the idea that a score given by a
number of judges can simply be averaged when a conflict occurs gives a
rather weird meaning to the score. It would make more sense for each
score to be stored in a different property, and averaged when they are
all present...</p>

<p>It could have been useful to show how more elaborate data (with
multiple properties) can be merge based on the identity of the client
(the book passes a client id but does not use it afterwards, so it
might not be possible to retrieve it).</p>

<h3>Pre and post-commit hooks</h3>

<p>Riak allows code to be executed before and after changes on the
database. This is similar to checks and triggers in PostgreSQL, but
the post-commit hooks are more powerful as they can perform pretty
much anything (although I have not explored triggers in other
languages, such as Perl, Python, ... that PostgreSQL supports).</p>

<p>Note: when I tried the example, I had a 500 Internal Server Error
instead of the expected 403 Forbidden return code. I eventually
tracked it down to a bug in the Erlang base code; see my explanations
above to install and patch Riak.</p>

<h3>Indexing and Searching</h3>

<p>Unlike what is stated in the first beta of the book, search is a
standard feature in Riak (at least 1.0.2). Just edit the <code>app.config</code>
file, look for search, and change the <code>enable</code> property to <code>true</code>.</p>

<p>Once search is enabled, it is recommended to change the index schema
to declare how to index and search various fields. Otherwise, the
search will not work as the book describes it. In particular, search
for a specific breed will not be case insensitive.</p>

<p>So, after enabling search in each server, I use the command
<code>dev1/bin/search-cmd install animals</code> to enable auto-indexing on data
updates (indexes can also be built from files, to the extent that you
have them).</p>

<p>Then I exported the default index with <code>dev1/bin/search-cmd get-schema
animals</code> (the output must be piped into a file).</p>

<p>I modified the file to add a declaration for the <code>breed</code> field,
following examples from the original
<a href="http://wiki.basho.com/Riak-Search---Schema.html">documentation</a>.</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>animals schema  (animals.json)</span> <a href='/downloads/code/7d7w/riak/animals.json'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="o">%%</span> <span class="nx">Schema</span> <span class="k">for</span> <span class="s1">&#39;animals&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>    <span class="nx">schema</span><span class="p">,</span>
</span><span class='line'>    <span class="p">[</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">version</span><span class="p">,</span> <span class="s2">&quot;1.1&quot;</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">n_val</span><span class="p">,</span> <span class="mi">3</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">default_field</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">whitespace_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>    <span class="p">],</span>
</span><span class='line'>    <span class="p">[</span>
</span><span class='line'>	<span class="p">{</span><span class="nx">field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;breed&quot;</span><span class="p">},</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">standard_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>	<span class="p">]},</span>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_num&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">integers</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_num&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">integer</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">integer_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_int&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">integers</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_int&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">integer</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">integer_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_dt&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">dates</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_dt&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">date</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">noop_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_date&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">dates</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_date&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">date</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">noop_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_txt&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">full</span> <span class="nx">text</span><span class="s2">&quot;</span>
</span><span class='line'><span class="s2">        {dynamic_field, [</span>
</span><span class='line'><span class="s2">            {name, &quot;</span><span class="o">*</span><span class="nx">_txt</span><span class="s2">&quot;},</span>
</span><span class='line'><span class="s2">            {type, string},</span>
</span><span class='line'><span class="s2">            {analyzer_factory, {erlang, text_analyzers, standard_analyzer_factory}}</span>
</span><span class='line'><span class="s2">        ]},</span>
</span><span class='line'>
</span><span class='line'><span class="s2">        %% Field names ending in &quot;</span><span class="nx">_text</span><span class="s2">&quot; are indexed as full text&quot;</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_text&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">string</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">standard_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Everything</span> <span class="k">else</span> <span class="nx">is</span> <span class="nx">a</span> <span class="nx">string</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">string</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">whitespace_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]}</span>
</span><span class='line'>    <span class="p">]</span>
</span><span class='line'><span class="p">}.</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Finally, I loaded the schema back into Riak with <code>dev1/bin/search-cmd set-schema animals animals.json</code></p>

<p>Now I can load the data as the book proposes (note that as I'm using
the standard <code>dev1</code> Riak server instead of a dedicated one, the port
is 8091 and not 8098).</p>

<p><code>
$ curl -X PUT http://127.0.0.1:8091/riak/animals/dragon \
-H "Content-Type: application/json" \
-d '{"nickname" : "Dragon", "breed" : "Briard", "score" : 1 }'
$ curl -X PUT http://127.0.0.1:8091/riak/animals/ace \
-H "Content-Type: application/json" \
-d '{"nickname" : "The Wonder Dog", "breed" : "German Shepherd", "score" : 3 }'
$ curl -X PUT http://127.0.0.1:8091/riak/animals/rtt \
-H "Content-Type: application/json" \
-d '{"nickname" : "Rin Tin Tin", "breed" : "German Shepherd", "score" : 4 }'
</code></p>

<p>And now the output of a search is as expected:</p>

<p>```
$ curl http://localhost:8091/solr/animals/select?q=breed:shepherd
&lt;?xml version="1.0" encoding="UTF-8"?>
<response>
  <lst name="responseHeader"></p>

<pre><code>&lt;int name="status"&gt;0&lt;/int&gt;
&lt;int name="QTime"&gt;1&lt;/int&gt;
&lt;lst name="params"&gt;
  &lt;str name="indent"&gt;on&lt;/str&gt;
  &lt;str name="start"&gt;0&lt;/str&gt;
  &lt;str name="q"&gt;breed:shepherd&lt;/str&gt;
  &lt;str name="q.op"&gt;or&lt;/str&gt;
  &lt;str name="filter"&gt;&lt;/str&gt;
  &lt;str name="df"&gt;value&lt;/str&gt;
  &lt;str name="wt"&gt;standard&lt;/str&gt;
  &lt;str name="version"&gt;1.1&lt;/str&gt;
  &lt;str name="rows"&gt;2&lt;/str&gt;
&lt;/lst&gt;
</code></pre>

<p>  </lst>
  <result name="response" numFound="2" start="0" maxScore="0.353553"></p>

<pre><code>&lt;doc&gt;
  &lt;str name="id"&gt;ace
  &lt;/str&gt;
  &lt;str name="breed"&gt;German Shepherd
  &lt;/str&gt;
  &lt;str name="nickname"&gt;The Wonder Dog
  &lt;/str&gt;
  &lt;str name="score"&gt;3
  &lt;/str&gt;
&lt;/doc&gt;
&lt;doc&gt;
  &lt;str name="id"&gt;rtt
  &lt;/str&gt;
  &lt;str name="breed"&gt;German Shepherd
  &lt;/str&gt;
  &lt;str name="nickname"&gt;Rin Tin Tin
  &lt;/str&gt;
  &lt;str name="score"&gt;4
  &lt;/str&gt;
&lt;/doc&gt;
</code></pre>

<p>  </result>
</response>
```</p>

<h2>Exercises</h2>

<h3>Indexing on score</h3>

<p>For this I first modified the index schema again:</p>

<p><div class='bogus-wrapper'><notextile><figure class='code'><figcaption><span>animals schema improved  (animals-score.json)</span> <a href='/downloads/code/7d7w/riak/animals-score.json'>download</a></figcaption>
 <div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
</pre></td><td class='code'><pre><code class='js'><span class='line'><span class="o">%%</span> <span class="nx">Schema</span> <span class="k">for</span> <span class="s1">&#39;animals&#39;</span>
</span><span class='line'>
</span><span class='line'><span class="p">{</span>
</span><span class='line'>    <span class="nx">schema</span><span class="p">,</span>
</span><span class='line'>    <span class="p">[</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">version</span><span class="p">,</span> <span class="s2">&quot;1.1&quot;</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">n_val</span><span class="p">,</span> <span class="mi">3</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">default_field</span><span class="p">,</span> <span class="s2">&quot;value&quot;</span><span class="p">},</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">whitespace_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>    <span class="p">],</span>
</span><span class='line'>    <span class="p">[</span>
</span><span class='line'>	<span class="p">{</span><span class="nx">field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;breed&quot;</span><span class="p">},</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">standard_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>	<span class="p">]},</span>
</span><span class='line'>	<span class="p">{</span><span class="nx">field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;score&quot;</span><span class="p">},</span>
</span><span class='line'>		<span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">integer</span><span class="p">}</span>
</span><span class='line'>	<span class="p">]},</span>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_num&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">integers</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_num&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">integer</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">integer_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_int&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">integers</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_int&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">integer</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">integer_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_dt&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">dates</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_dt&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">date</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">noop_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_date&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">dates</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_date&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">date</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">noop_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Field</span> <span class="nx">names</span> <span class="nx">ending</span> <span class="k">in</span> <span class="s2">&quot;_txt&quot;</span> <span class="nx">are</span> <span class="nx">indexed</span> <span class="nx">as</span> <span class="nx">full</span> <span class="nx">text</span><span class="s2">&quot;</span>
</span><span class='line'><span class="s2">        {dynamic_field, [</span>
</span><span class='line'><span class="s2">            {name, &quot;</span><span class="o">*</span><span class="nx">_txt</span><span class="s2">&quot;},</span>
</span><span class='line'><span class="s2">            {type, string},</span>
</span><span class='line'><span class="s2">            {analyzer_factory, {erlang, text_analyzers, standard_analyzer_factory}}</span>
</span><span class='line'><span class="s2">        ]},</span>
</span><span class='line'>
</span><span class='line'><span class="s2">        %% Field names ending in &quot;</span><span class="nx">_text</span><span class="s2">&quot; are indexed as full text&quot;</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*_text&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">string</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">standard_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]},</span>
</span><span class='line'>
</span><span class='line'>        <span class="o">%%</span> <span class="nx">Everything</span> <span class="k">else</span> <span class="nx">is</span> <span class="nx">a</span> <span class="nx">string</span>
</span><span class='line'>        <span class="p">{</span><span class="nx">dynamic_field</span><span class="p">,</span> <span class="p">[</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">name</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">type</span><span class="p">,</span> <span class="nx">string</span><span class="p">},</span>
</span><span class='line'>            <span class="p">{</span><span class="nx">analyzer_factory</span><span class="p">,</span> <span class="p">{</span><span class="nx">erlang</span><span class="p">,</span> <span class="nx">text_analyzers</span><span class="p">,</span> <span class="nx">whitespace_analyzer_factory</span><span class="p">}}</span>
</span><span class='line'>        <span class="p">]}</span>
</span><span class='line'>    <span class="p">]</span>
</span><span class='line'><span class="p">}.</span>
</span></code></pre></td></tr></table></div></figure></notextile></div></p>

<p>Then I reentered the data (see above) to get it indexed.</p>

<p>Finally, I input a query in Firefox, to let it figure out the HTTP
escape characters. The result is used below with curl (both outputs
were identical):</p>

<p>```
curl http://localhost:8091/solr/animals/select?q=score:%5B2%20TO%204%5D
&lt;?xml version="1.0" encoding="UTF-8"?>
<response>
  <lst name="responseHeader"></p>

<pre><code>&lt;int name="status"&gt;0&lt;/int&gt;
&lt;int name="QTime"&gt;3&lt;/int&gt;
&lt;lst name="params"&gt;
  &lt;str name="indent"&gt;on&lt;/str&gt;
  &lt;str name="start"&gt;0&lt;/str&gt;
  &lt;str name="q"&gt;score:[2 TO 4]&lt;/str&gt;
  &lt;str name="q.op"&gt;or&lt;/str&gt;
  &lt;str name="filter"&gt;&lt;/str&gt;
  &lt;str name="df"&gt;value&lt;/str&gt;
  &lt;str name="wt"&gt;standard&lt;/str&gt;
  &lt;str name="version"&gt;1.1&lt;/str&gt;
  &lt;str name="rows"&gt;2&lt;/str&gt;
&lt;/lst&gt;
</code></pre>

<p>  </lst>
  <result name="response" numFound="2" start="0" maxScore="0.00000e+0"></p>

<pre><code>&lt;doc&gt;
  &lt;str name="id"&gt;ace
  &lt;/str&gt;
  &lt;str name="breed"&gt;German Shepherd
  &lt;/str&gt;
  &lt;str name="nickname"&gt;The Wonder Dog
  &lt;/str&gt;
  &lt;int name="score"&gt;3
  &lt;/int&gt;
&lt;/doc&gt;
&lt;doc&gt;
  &lt;str name="id"&gt;rtt
  &lt;/str&gt;
  &lt;str name="breed"&gt;German Shepherd
  &lt;/str&gt;
  &lt;str name="nickname"&gt;Rin Tin Tin
  &lt;/str&gt;
  &lt;int name="score"&gt;4
  &lt;/int&gt;
&lt;/doc&gt;
</code></pre>

<p>  </result>
</response>
```</p>

<h3>Distributed Riak</h3>

<p>I don't see much of a problem with this, I'll give it a try when I get
home. There are already 4 different development servers easily
available in a standard Riak package, so using one on each machine
would do the trick. Of course, when adding the various servers to the
ring, the hostname must be changed, but this should really be a piece
of cake.</p>

<h2>Wrapping up Riak</h2>

<p>It seems Riak is a low level data store that trades easy of use and
packaged features for high availability and flexible for
performance cost.</p>

<p>I can see how this could be in theory appealing in some circumstances,
but I don't really see how to put such a framework to use.</p>

<p>SQL databases don't just come with a nice engine optimised for
relational queries; they also come with decades of experience, general
guidelines for schema design and domain specific schema organisations.</p>

<p>All these contribute to make the meaning of the data clear and
useful. With Riak the quorum option can be used as a decision
mechanism, but beside it is not obvious what meaning to give to a
piece of information that has two or more concurrent variations. And
of course consistency can no longer be implemented on more than one key
no matter what.</p>

<p>This means that a solution based on Riak will be significantly
different from one using an SQL database. Perhaps it would feel more
natural to an object oriented programmer who thinks in terms of
objects and object references. But even in this context, the vector
clock based resolution should still be difficult to design properly.</p>

<p>The book in this regard fails even to acknowledge the problem; it
concentrates on tools but does not give any framework to guide in the
design of a solution built on Riak.</p>

<p>I still has to check the
<a href="http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html">Amazon Dynamo Paper</a>
and the other relevant literature, so I have hope yet I will come up with a
reasonable understanding of all this.</p>
]]></content>
  </entry>
  
</feed>
