<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mathematics | Wakatta!]]></title>
  <link href="http://blog.wakatta.jp/blog/categories/mathematics/atom.xml" rel="self"/>
  <link href="http://blog.wakatta.jp/"/>
  <updated>2012-03-13T18:44:18+09:00</updated>
  <id>http://blog.wakatta.jp/</id>
  <author>
    <name><![CDATA[Frédéric Dumont]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 2 Basics]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/03/10/concrete-mathematics-chapter-2-basics/"/>
    <updated>2012-03-10T11:10:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/03/10/concrete-mathematics-chapter-2-basics</id>
    <content type="html"><![CDATA[<p>This second batch of exercises builds on the
<a href="/blog/2012/02/28/concrete-mathematics-chapter-2-warmups/">previous one</a>. Once
again, there are no complex manipulations, and very often the solution
just follows from the definitions.</p>

<!--more-->


<h2>Basics</h2>

<h3>$\sum_{0\le k\lt n}(a_{k+1}-a_k)b_k$</h3>

<p>To show that</p>

<div markdown="0">
\begin{align}
\sum_{0\le k\lt n}(a_{k+1}-a_k)b_k &amp; = a_n b_n - a_0 b_0 - \sum_{0 \le k \lt n} a_{k+1}(b_{k+1} - b_k)&amp;&amp;n\ge 0\\\\
\end{align}
</div>


<p>I start by rewriting the sum in the right side of the equation:</p>

<div markdown="0">
\begin{align}
\sum_{0 \le k \lt n} a_{k+1}(b_{k+1} - b_k) &amp; = \sum_{0 \le k \lt n} (a_{k+1}b_{k+1} +  a_{k+1} b_k)\\\\
&amp; = \sum_{0 \le k \lt n} a_{k+1}b_{k+1} +  \sum_{0 \le k \lt n} a_{k+1} b_k&amp;&amp;\text{associative law}\\\\
&amp; = \sum_{0 \le k-1 \lt n} a_k b_k +  \sum_{0 \le k \lt n} a_{k+1} b_k&amp;&amp;k\leftarrow k-1\\\\
&amp; = \sum_{1 \le k \le n} a_k b_k +  \sum_{0 \le k \lt n} a_{k+1} b_k\\\\
\end{align}
</div>


<p>This latest value can now be put back into the original right:</p>

<div markdown="0">
\begin{align}
a_n b_n - a_0 b_0 - \sum_{1 \le k \le n} a_k b_k +  \sum_{0 \le k \lt n} a_{k+1} b_k &amp; = \sum_{0\le k \lt n} a_{k+1} b_k - (a_0 b_0 + \sum_{1 \le k \le n} a_k b_k - a_n b_n)\\\\
&amp; = \sum_{0\le k \lt n} a_{k+1} b_k - \sum_{0\le k \lt n} a_k b_k\\\\
&amp; = \sum_{0\le k \lt n} (a_{k+1} b_k - a_k b_k)\\\\
&amp; = \sum_{0\le k \lt n} (a_{k+1} - a_k) b_k\\\\
\end{align}
</div>


<p>which is indeed the left side of the equation (the but-last step is
permitted under the associative law, but that didn't fit in the margin).</p>

<h3>$p(k) = k + (-1)^k c$</h3>

<p>It is clear that there is a single $p(k)$ for every possible (integer)
$k$. So I need to show that for every $m$, there is a single $k$ such
that $p(k)=m$, defining $p^{-1}$.</p>

<p>The book method is smart, mine clearly less so, but as far as I can
tell, still correct: for $m$, I consider $m-c$ and $m+c$. The
difference is $2c$, so they're either both even, or both
odd.</p>

<p>If they're both even, then $m-c+(-1)^{m-c}c=m$, so $k=m-c$. If they're
both odd, then $m+c+(-1)^{m+c}c=m$, so $k=m+c$. So $k$ is always well
defined for every $m$, and $p$ is indeed a permutation.</p>

<h3>$\sum_{k=0}^n (-1)^k k^2$</h3>

<p>While I found the closed formula for the sum, I could not do it with
the repertoire method.</p>

<p>Solving the sum is not really difficult (although a little bit than
the repertoire method, if you know how to do the latter); one way is
to solve the positive and negative sums separately (they can be broken
down to already solved sums); another one is to compute the sum of an
even number of terms (one positive and one negative), then to compute
sums of odd number of terms (by adding a term to the previous
solution), and finally combining both to find the closed formula.</p>

<p>In both attempts above, I tried to remove the $(-1)^k$ factor from the
terms; when using the repertoire method I tried to do the same, which
is why I failed.</p>

<p>The repertoire method relies on a good intuition: one must have a
sense of general shape of the parametric functions. In retrospect, it
seems obvious, but I just couldn't see it, blinded as I was by$(-1)^k$.</p>

<p>Expressing the sum as a recurrence is easy:</p>

<div markdown="0">
\begin{align}
R_0 &amp; = 0\\\\
R_n &amp; = R_{n-1} + (-1)^n n^2\\\\
\end{align}
</div>


<p>Also, looking at the first few terms of the sum,
$-1, 3, -6, 10, -15, \dots$, it is natural to consider solutions of
the form $(-1)^n F(n)$; it is a little bit trickier to see where a good
generalisation of the recurrence above should put the additional
terms:</p>

<div markdown="0">
\begin{align}
R_0 &amp; = \alpha\\\\
R_n &amp; = R_{n-1} + (-1)^n \left(\beta + \gamma n + \delta n^2 \right)\\\\
\end{align}
</div>


<p>With such a form, plugging in solutions $(-1)^nF(n)$ will
simplify to $F(n) = \beta + \gamma n + \delta n^2 - F(n-1)$.</p>

<p>At this stage, it becomes very easy to find the $A(n)$, $B(n)$, $C(n)$
and $D(n)$ functions (the latter being the solution we are looking
for). In fact, if all you care about is $D(n)$, then it is enough to
use $R_n = (-1)^n n$ and $R_n = (-1)^n n^2$:</p>

<h4>$R_n = (-1)^n n$</h4>

<div markdown="0">
\begin{align}
R_0 &amp; = 0&amp;&amp;\alpha = 0\\\\
n &amp; = \beta + \gamma n + \delta n^n - n + 1\\\\
2n - 1 &amp; = \beta + \gamma n&amp;&amp;\beta = -1, \gamma = 2\\\\
\end{align}
</div>


<p>which gives $-B(n)+2C(n) = (-1)^n n$.</p>

<h4>$R_n = (-1)^n n^2$</h4>

<div markdown="0">
\begin{align}
R_0 &amp; = 0&amp;&amp;\alpha = 0\\\\
n^2 &amp; = \beta + \gamma n + \delta n^2 - (n-1) ^2\\\\
2 n^2 - 2n + 1 &amp; = \beta + \gamma n + \delta n^2&amp;&amp;\beta = 1, \gamma = -2, \delta = 2\\\\
\end{align}
</div>


<p>which gives $B(n)-2C(n)+2D(n) = (-1)^n n^2$. Combining with the
previous answer, we have $2D(n) = (-1)^n (n^2-n)$, or
$D(n) = (-1)^n \frac{n^2-n}{2}$.</p>

<h4>Wrapping up this exercise</h4>

<p>In hindsight, these steps could have helped me solve this
exercise as intended:</p>

<ul>
<li>compute the first few terms to see if there is something obvious
about their shape; in this case, the $(-1)^n$ factor</li>
<li>at first, write the recurrence equations as simply as possible,
with all the "inconvenient" parts; comparing them to the "shapes"
identified in the previous step might give some insight about the
general solutions, and possibly removed these difficult parts</li>
<li>only then, consider how to generalise the recurrence equations. The
base case is always $R_0 = \alpha$; the recurrent case should add
parameters to each term, and additional terms (with their own
parameters) to complete some basic classes of problems (for instance,
if there are any polynomial, there should be a term for each power
smaller than the largest power of the original problem; another basic
class is the generalised radix-based Josephus problem)</li>
<li>each class of problems can be solved independently; this makes it
easier to find potential solutions and to combine them.</li>
</ul>


<h3>$\sum_{k=1}^n k2^k$</h3>

<p>Not overly complicated; at least the introduction of $j$ is not a
mystery (unlike the next exercise).</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\le n}k 2^k &amp; = \sum_{1\le k\le n} 2^k \sum_{1\le j\le k}1\\\\
&amp; = \sum_{1\le k\le n} \sum_{1\le j\le k} 2^k\\\\
&amp; = \sum_{1\le j\le k \le n} 2^k\\\\
&amp; = \sum_{1\le j\le n} \sum_{j\le k\le n}2^k\\\\
\end{align}
</div>


<p>The inner sum can be rewritten as</p>

<div markdown="0">
\begin{align}
\sum_{j\le k\le n}2^k &amp; = \sum_{1\le k\le n}2^k - \sum_{1\le k\lt j}2^k\\\\
&amp; = 2^{n+1} - 2 - 2^j + 2\\\\
&amp; = 2^{n+1} - 2^j\\\\
\end{align}
</div>


<p>Here I use the already known
sum $\sum 2^k$. Putting this last result
in the original sum</p>

<div markdown="0">
\begin{align}
\sum_{1\le j\le n} 2^{n+1} - 2^j &amp; = n2^{n+1} - (2^{n+1} -2)\\\\
\end{align}
</div>


<h3>$\sum_{k=1}^n k^3$</h3>

<p>It took me some time to convince myself that the original rewrite was
legitimate; eventually I did it by induction (the book version is much
shorter, and once you see it, much easier). Clearly it works for
$n=1$, so assuming it does for $n-1$, we have</p>

<div markdown="0">
\begin{align}
2\sum_{1\le j\le k\le n} jk &amp; = 2\sum_{1\le j\le k\le n-1} jk + 2\sum_{1\le j\le k=n} jk\\\\
&amp; = \sum_{1\le k\lt n}(k^3+k^2) + 2n\sum_{1\le j\le n} j\\\\
&amp; = \sum_{1\le k\lt n}(k^3+k^2) + n^2(n+1)\\\\
&amp; = \sum_{1\le k\lt n}(k^3+k^2) + n^3+n^2\\\\
\end{align}
</div>


<p>So the rewrite is correct. At this stage, (2.33) pretty much finishes it:</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\le n}(k^3+k^2) &amp; = (\sum_{1\le k\le n}k)+\sum_{1\le k\le n}k^2\\\\
\end{align}
</div>


<p>so $\sum_{1\le k\le n}k^3=\frac{n^2(n+1)^2}{4}$.</p>

<h3>$\frac{x^{\underline m}}{(x-n)^{\underline m}} = \frac{x^{\underline n}}{(x-m)^{\underline n}}$</h3>

<p>This follows directly from
$\frac{a}{b} = \frac{c}{d} \implies ad = bc$, and the use of equation (2.52).</p>

<h3>Rising and Falling Factorial Powers Conversions</h3>

<p>I'll just do the conversion from raising factorial power to falling
factorial power; the other conversion is just the same.</p>

<p>$x^{\overline m} = \frac{1}{(x-1)^{\underline m}}$ follows from (2.51)
and (2.52).</p>

<p>For the other equalities, by induction on $m$, and using (2.52) and
its raising factorial powers equivalent:</p>

<div markdown="0">
\begin{align}
x^{\underline m} &amp; = x^{\underline{m-1}}(x-m+1)\\\\
&amp; = x^{\underline 1}(x-1)^{\underline{m-1}}\\\\
&amp; = x(x-1)^{\underline{m-1}}\\\\
x^{\overline m} &amp; = x^{\overline{m-1}}(x+m-1)\\\\
&amp; = x^{\overline 1}(x+1)^{\overline{m-1}}\\\\
&amp; = x(x+1)^{\overline{m-1}}\\\\
\end{align}
</div>


<h4>Base case $m=0$</h4>

<p>They all follow from definition:</p>

<div markdown="0">
\begin{align}
x^{\overline 0} &amp; = 1\\\\
(-1)^0 (-x)^{\underline 0} &amp; = 1\\\\
(x+0-1)^{\underline 0} &amp; = 1\\\\
\end{align}
</div>


<h4>Other positive $m$</h4>

<p>Assuming the relations hold for all $k, 0\le k\lt m$:</p>

<div markdown="0">
\begin{align}
(-1)^m(-x)^{\underline m} &amp; = -\left((-1)^{m-1}(-x)^{\underline{m-1}}(-x-m+1)\right)\\\\
&amp; = (x^{\overline{m-1}})(x+m-1)\\\\
(x+m-1)^{\underline m} &amp; = (x+m-1)^{\underline{m-1}}x\\\\
&amp; = (x+1+(m-1)-1)^{\underline{m-1}}x\\\\
&amp; = (x+1)^{\overline{m-1}}x\\\\
\end{align}
</div>


<h4>Negative $m$</h4>

<p>Using the recurrence relations derived from (2.52) and its raising
factorial power equivalent:</p>

<div markdown="0">
\begin{align}
x^{\underline m} &amp; = x^{\underline{(m+1)+(-1)}}\\\\
&amp; = x^{\underline{-1}}(x+1)^{\underline{m+1}}\\\\
&amp; = \frac{(x+1)^{\underline{m+1}}}{x+1}\\\\
&amp; = x^{\underline{m+1}}(x-m-1)^{\underline{-1}}\\\\
&amp; = \frac{x^{\underline{m+1}}}{x-m}\\\\
x^{\overline m} &amp; = x^{\overline{(m+1)+(-1)}}\\\\
&amp; = x^{\overline{-1}}(x-1)^{\overline{m+1}}\\\\
&amp; = \frac{(x-1)^{\overline{m+1}}}{x-1}\\\\
&amp; = x^{\overline{m+1}}(x+m+1)^{\overline{-1}}\\\\
&amp; = \frac{x^{\overline{m+1}}}{x+m}\\\\
\end{align}
</div>


<p>Assuming the relations hold for all $k, m\lt k\le 0$:</p>

<div markdown="0">
\begin{align}
(-1)^m(-x)^{\underline m} &amp; = -\frac{(-1)^{m+1}(-x)^{\underline{m+1}}}{-x-m}\\\\
&amp; = \frac{x^{\overline{m+1}}}{x+m}\\\\
(x+m-1)^{\underline m} &amp; = \frac{(x+m)^{\underline{m+1}}}{x+m-1-m}\\\\
&amp; = \frac{(x-1)^{\overline{m+1}}}{x-1}\\\\
\end{align}
</div>


<p>So the main difficulties is to derive two equalities from (2.52) (four
if we count the negative cases as well), and the identification of the
recurrence equation in the induction step (especially for
$(x+m-1)^{\underline{m\pm 1}}$).</p>

<h3>Absolute Convergence of Complex Sums</h3>

<p>I suppose I could say it follows directly from the equivalence of the
metric functions (if my memory of metric space terminology is correct).</p>

<p>More basically, the equivalence of the propositions follows from the
relationships based on the hypotenuse formula:
$\sqrt{(Rz)^2+(Iz)^2}\le |Rz| + |Iz|$, so the absolute convergence of
the real and imaginary parts implies the absolute convergence of the
absolute value. Conversely, $|Rz|,|Iz|\le\sqrt{(Rz)^2+(Iz)^2}$, so the
absolute convergence of the absolute value also implies the absolute
convergence of both the real and imaginary parts.</p>

<h3>Wrapping up</h3>

<p>This time, I found a solution to all the exercises, which is a
progress of some sort. I still have trouble with the repertoire method,
or perhaps not with the method itself but in identifying suitable
generalisations and candidate solutions. This is something that can
only be developed with practice, so I just have to be patient and
keep trying (I hope I'll get there eventually).</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 2 Warmups]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/28/concrete-mathematics-chapter-2-warmups/"/>
    <updated>2012-02-28T19:18:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/28/concrete-mathematics-chapter-2-warmups</id>
    <content type="html"><![CDATA[<p>This first batch of exercises is meant to develop familiarity with
the various concepts and notations introduced in this chapter. There
is no complex manipulation, but the trick is to be aware of the often
unmentioned assumptions about the precise meaning of the expressions.</p>

<!--more-->


<h2>Warmups</h2>

<h3>$\sum_{k=4}^0 q_k$</h3>

<p>The meaning of such an expression is not clear, so there is no real
way to fail this exercise.</p>

<p>A first interpretation, maybe the common one, is that the sum is zero
because the range is empty. In other words, the sum is
$\sum_{4\le k\le 0} q_k$.</p>

<p>A second interpretation, perhaps for those used to programming
languages with very flexible loops could argue that the sum is
$q_4 + q_3 + q_2 + q_1 + q_0$.</p>

<p>I toyed briefly with a negative sum, similar to integrals with
reversed bounds, but I did not come up with the nice book solution
of $\sum_{k=m}^n = \sum_{k\le n} - \sum_{k\lt m}$, which is consistent
with and extends the first interpretation.</p>

<h3>Simplify $x([x\gt 0] - [x\lt 0])$</h3>

<p>It is easy to see that the expression has the same value as $|x|$:</p>

<div markdown="0">
\begin{align}
x([x\gt 0] - [x\lt 0]) &amp; = x (1-0)&amp;&amp;\text{when \(x\gt 0\)}\\\\
&amp; = x\\\\
x([x\gt 0] - [x\lt 0]) &amp; = x (0-1)&amp;&amp;\text{when \(x\lt 0\)}\\\\
&amp; = -x\\\\
x([x\gt 0] - [x\lt 0]) &amp; = 0&amp;&amp;\text{when \(x = 0\)}\\\\
\end{align}
</div>


<h3>Writing out sums</h3>

<p>The first one is easy:</p>

<div markdown="0">
\begin{align}
\sum_{0\le k\le 5}a_k = a_0+a_1+a_2+a_3+a_4+a_5\\\\
\end{align}
</div>


<p>The second one is tricky, is more than one way. One problem is that
$k$ is not explicitly defined, and I had assumed it was a natural,
when the authors thought of it as a integer; now the latter is in line
with the book conventions, so I was wrong and had missing terms. The
right answer is:</p>

<div markdown="0">
\begin{align}
\sum_{0\le k^2 \le 5}a_k = a_4 + a_1 + a_0 + a_1 + a_4\\\\
\end{align}
</div>


<h3>Triple Sum</h3>

<p>Here it is important to restrict the bounds as much as possible (but
no more); otherwise there is a risk of introducing spurious terms.</p>

<div markdown="0">
\begin{align}
\sum_{1\le i \lt j \lt k \le n}a_{ijk} &amp; = \sum_{i=1}^2 \sum_{j=i+1}^3 \sum_{k=j+1}^4 a_{ijk}\\\\
&amp; = \left((a_{123} + a_{124}) + a_{134} \right) + a_{234}\\\\
&amp; = \sum_{k=3}^4 \sum_{j=2}^{k-1} \sum_{i=1}^{j-1} a_{ijk}\\\\
&amp; = a_{123}+\left(a_{124} + (a_{134} + a_{234})\right)\\\\
\end{align}
</div>


<p>The terms appear in the same order, but are grouped in sums differently.</p>

<h3>Incorrect derivation</h3>

<p>The problem is the step</p>

<div markdown="0">
\begin{align}
\sum_{j=1}^n \sum_{k=1}^n = \frac{a_j}{a_k}\sum_{k=1}^n \sum_{k=1}^n \frac{a_k}{a_k}\\\\
\end{align}
</div>


<p>$k$ is already bound in the inner sum, so it is invalid to replace $j$
by $k$ in the outer.</p>

<h3>$\sum_k [1\le j\le k\le n]$</h3>

<p>This can be worked explicitly:</p>

<div markdown="0">
\begin{align}
\sum_k [1 \le j \le k \le n] &amp = \sum_k [1 \le j \le n] [j \le k \le n]\\\\
&amp; = \sum_{j\le k \le n} [1 \le j \le n]\\\\
&amp; = [1 \le j \le n] \sum_{j\le k \le n} 1\\\\
&amp; = [1 \le j \le n] (n-j+1)\\\\
\end{align}
</div>


<h3>$\bigtriangledown f(x)$</h3>

<p>The result is not surprising:</p>

<div markdown="0">
\begin{align}
\bigtriangledown x^{\overline{m}} &amp; = x^{\overline{m}} - (x-1)^{\overline{m}}\\\\
&amp; = x(x+1)\cdots(x+m-1) - (x-1)x\cdots(x+m-2)\\\\
&amp; = x(x+1)\cdots(x+m-2)(x+m-1-(x-1))\\\\
&amp; = m x^{\overline{m-1}}\\\\
\end{align}
</div>


<p>So $\bigtriangledown f(x)$ is the difference operator to use with
rising factorials.</p>

<h3>$0^{\overline{m}}$</h3>

<p>Clearly, when $m\lt 0$, $0^{\overline{m}} = 0$; when $m = 0$,
$0^{\overline{m}} = 1$ (to make the expression
$x^{\underline{1+0}}=x^{\underline 1}(x-1)^{\underline 0}$ work when $x=1$); I
had forgotten about $m&lt;0$, which was perhaps the easiest case, as $\frac{1}{m!}$
(it follows directly from the definition of falling factorials with negative
powers).</p>

<h3>Law of exponents for rising factorials</h3>

<p>It is easy to see that $x^{\overline{m+n}} = x^{\overline m}(x+m)^{\overline n}$:</p>

<div markdown="0">
\begin{align}
x^{\overline{m+n}} &amp; = x\cdots(x+m-1)(x+m)\cdots(x+m+n-1)\\\\
&amp; = \left( x\cdots(x+m-1) \right) \left( (x+m)\cdots(x+m+n-1) \right)\\\\
&amp; = x^{\overline m}(x+m)^{\overline n}\\\\
\end{align}
</div>


<p>From there, the value of rising factorials for negative powers follows quickly:</p>

<div markdown="0">
\begin{align}
1 = x^{\overline{-n+n}} &amp; = x^{\overline{-n}} (x-n)^\overline{n}\\\\
x^{\overline{-1}} &amp; = \frac{1}{(x-n)^\overline{n}}\\\\
&amp; = \frac{1}{(x-n)\cdots(x-1)}\\\\
&amp; = \frac{1}{(x-1)^{\underline{n}}}\\\\
\end{align}
</div>


<h3>Symmetric difference of a product</h3>

<p>To start, I quickly looked up the proof of the original derivative
product rule on
<a href="http://en.wikipedia.org/wiki/Product_rule#Proof_of_the_product_rule">Wikipedia</a>;
the geometric nature of the proof was illuminating (I believe I was
taught the so called
<a href="http://en.wikipedia.org/wiki/Product_rule#A_Brief_Proof">Brief Proof</a>
both in high-school and at university).</p>

<p>This geometric proof can be used for both the infinite and the finite
calculus, and its symmetric nature (there are two ways to compute the
area of the big rectangle:
$f(x)g(x)+(f(w)-f(x))g(w) + f(x)(g(w)-g(x))$ and
$f(x)g(x)+f(w)(g(w)-g(x)) + (f(w)-f(x))g(x)$) can be used in the
finite case. The symmetry (and equality) is restored
because in the infinite calculus, $\lim_{w\rightarrow x}f(w) = f(x)$
and $\lim_{w\rightarrow x}g(w) = g(x)$, a restoration that is not
possible in the finite calculus.</p>

<p>However, the equivalent finite calculus formulas,
$\bigtriangleup(uv) = u\bigtriangleup v + Ev\bigtriangleup u$ and
$\bigtriangleup(uv) = Eu\bigtriangleup v + v\bigtriangleup u$, have
together the symmetry they lack on their own.</p>

<h3>Wrapping up</h3>

<p>OK, that was not entirely bad (two small mistakes, both about negative
numbers blindness). Next step, the basic exercises.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 2 Notes]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/27/concrete-mathematics-chapter-2-notes/"/>
    <updated>2012-02-27T10:54:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/27/concrete-mathematics-chapter-2-notes</id>
    <content type="html"><![CDATA[<p>After a long but busy silence, I have now a few notes on the second
chapter, Sums. As with
<a href="/blog/2012/01/06/concrete-mathematics-chapter-1-notes/">Chapter 1</a>,
these are nothing revolutionary; just some clarifications of the
points that were not obvious to me, as well as other, random
observations.</p>

<!--more-->


<p>Overall, this chapter felt less overwhelming than the first, despite
being much longer and introducing very powerful techniques. I have yet
to do the exercises, though, so I may still revise this judgement.</p>

<h3>Notation</h3>

<p>The authors mentions that the Sigma-notation is "... impressive to
family and friends". I can confirm that assessment.</p>

<p>The remark on keeping bounds simple actually goes beyond resisting
"premature optimisation", that is, removing terms just because they
are equal to zero. Sometimes, it is worth adding a zero term if it
simplifies the bounds. Such a trick is used in solving
$\sum_{1\le j\lt k\le n} \frac{1}{k-j}$, and I'll get back to this
point when I go over this solution.</p>

<p>The Iverson notation (or Iversonian) is a very useful tool, as is the
general Sigma-notation. About the latter, it already simplifies
variable changes a lot, but I found it useful (and less error prone)
to always write the variable change on the right margin (for instance
as $k \leftarrow k+1$) and to keep that change as the only one in a
given line of the rewrite; otherwise, no matter how trivial the
change, any error I make at that time will be hard to locate (I know;
I tried).</p>

<h3>Sums and Recurrence</h3>

<p>First we see how easy it is to use the repertoire method to build
solutions to common (or slightly generalised) sums. The only problem
with the repertoire method is it requires a well furnished repertoire
of solutions to basic recurrences; I'm sure I would never have come up
with the radix-change solution to the generalised Josephus
problem. And given that there is an infinite number of functions one
could try, a more directed method is sometimes necessary.</p>

<p>This section also shows how to turn some recurrence equations (such as
the Tower of Hanoi one) into a sum; this method involve a choice
($s_1$ can be any non-zero value), which could either simplify or
complicate the solution. I haven't done the exercises yet, so I don't
know to what extent the choice is obvious or tricky.</p>

<p>Finally it shows how to turn a recurrence expressed as a sum of all
the previous values into a simpler recurrence by computing the
difference between two successive values. This is one instance of a
more general simplification using a linear combination of a few
successive values.</p>

<h3>Manipulation of Sums</h3>

<p>Unsurprisingly, sums have the same basic properties as common
additions: distributive, associative and commutative laws. Only the
latter is really tricky, as it involves a change to the index
variable. As mentioned above, I found useful to make such changes
really clear and isolated in any reasoning.</p>

<p>With these laws confirmed, it is possible to build the first method
for solving sums: the perturbation method. It is very simple, and
while it does not always work, when it does it is very quick.</p>

<h3>Multiple Sums</h3>

<p>This is perhaps the first section where I had to slow down; basically
multiple sums are not different from simple sums, and manipulations
are defined by the distributive law, but index variable changes
(especially the rocky road variety) require special attention. This,
combined with "obvious" simplifications (obvious to the authors, and
sometimes in retrospect to the reader as well), gave me some
difficulties.</p>

<p>For instance, the solution to</p>

<div markdown="0">
\begin{align}
\sum_{1\le j\lt k\le n} \frac{1}{k-j}
\end{align}
</div>


<p>The index variable change $k \leftarrow k+j$ is explained as a
specific instance of the simplification of $k+f(j)$; more perplexing
are the ranges for $j$ and $k$ when the sum is replaced by a sum of sum:</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\le n} \sum_{1\le j \le n-k} \frac{1}{k}
\end{align}
</div>


<p>The range for $j$ is built from $1\le j$ and $k+j\le n$, so there is
nothing really strange here.</p>

<p>The range for $k$, however, looks like a typo: certainly the authors
meant $1\le k\lt n$. A margin graffiti confirms the range, but it does
not really explain it.</p>

<p>The fact is, it is safe to let $k\le n$ here, because the sum over $j$
when $k=n$ is zero: not only the expression
$\sum_{1\le j \le k-n = 0} \frac{1}{k}$ is zero because there is no
$j$ that can satisfies the range predicate, but the closed form
of this sum, $\frac{k-n}{k}$, is also zero when $k=n$.</p>

<p>With the closed form checked, it is safe to add extra terms to
simplify the range of $k$.</p>

<p>What happens if you don't see this possible simplification? As
expected, the answer remains the same:</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\lt n} \sum_{1\le j \le n-k} \frac{1}{k} &amp; = \sum_{1\le k\lt n} \frac{n-k}{k}\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} - \sum_{1\le k\lt n} \frac{k}{k}\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} - (n-1)\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} + \frac{n}{n} - n\\\\
&amp; = \sum_{1\le k\le n} \frac{n}{k} - n\\\\
&amp; = nH_n - n\\\\
\end{align}
</div>


<p>So to expend on the original advice of keeping the bounds as simple as
possible: sometimes it is possible to extend the bounds (in order to
simplify them), as long as the extra terms in closed form evaluate to
zero. If the extra terms are still defined as sums, just checking that
the range is empty might not be enough.</p>

<h3>General Methods</h3>

<p>A cool and fun section on the various ways to solve a given sum.</p>

<p>Method 0 is to look it up. This book, written before the rise of
Internet (I remember Internet in the early 1990's; most of it was still
indexed manually on the CERN index pages...), suggests a few books as
resources.</p>

<p>Fortunately, some of them have migrated to the
<a href="https://oeis.org/">Web</a>, which is a more suitable tool than books for
such knowledge; the combination of searches and instant updates is
hard to beat (a book remains best for a content that is mostly linear
and somewhat independent of time; a novel, or textbook, for
instance. References are better on Internet, free if possible, for
a subscription otherwise).</p>

<p>Method 1 is guessing then proving; proving in fact should be a
complement for all the other methods (except perhaps Method 0). Having
two independents proofs is always good.</p>

<p>Method 2 is the perturbation method. In this section example, we see
how an apparent failure can still be exploited by being imaginative.</p>

<p>Method 3 is the repertoire method. In this chapter it is usually much
simpler than in the first.</p>

<p>Method 4 uses calculus to get a first approximation, then uses other
methods to solve the equations for the error function.</p>

<p>Method 5 is a clever rewriting of the problem into a sum of sums;
like the repertoire method but unlike the others, it requires some
intuition to find a solution (perhaps more than the repertoire
method); I have bad memories of trying such a method to solve problems
at university, always somehow ending up right where I started. I guess
I will try other methods if I can.</p>

<p>Method 6 is the topic of the next section; method 7 is for another
chapter.</p>

<h3>Finite and Infinite Calculus</h3>

<p>This section was surprising and exciting, but not really that
complex. It really is a matter of adapting regular calculus reflexes to the
finite version. I have to see how it works in practice.</p>

<p>One thing that is causing me some trouble is the falling-power version
of the law of exponents:</p>

<div markdown="0">
\begin{align}
x^{\underline{m+n}} &amp; = x^{\underline m}(x-m)^{\underline n}\\\\
\end{align}
</div>


<p>While the rule is easy to prove and to remember, it is less easy than
the general one to recognise in practice; I failed to see it when it
came up in the solution to</p>

<div markdown="0">
\begin{align}
\Sigma xH_x\delta x\\\\
\end{align}
</div>


<p>Worse, even the explanation in the book, I had to write it down, play
with it, before seeing it.</p>

<p>So I'm thinking about a notation that would bring out the rule more
clearly, an extension of the <em>shift operator</em> $E$:</p>

<div markdown="0">
\begin{align}
E_k f(x) &amp; = f(x-k)\\\\
\end{align}
</div>


<p>This would turn the exponent law into</p>

<div markdown="0">
\begin{align}
x^{\underline{m+n}} &amp; = x^{\underline m} E_m x^{\underline n}\\\\
\end{align}
</div>


<p>Whether this is useful, or whether I'll get used to the original
notation anyway, we'll see in the exercises...</p>

<h3>Infinite Sums</h3>

<p>The last section is about infinite sums. The authors quite sensibly
restrict the scope to absolutely convergent sums, which have the
advantage that the three basic laws and the manipulations they allow
are still valid.</p>

<p>Once again, this was not overly difficult; the only point I had
trouble understanding was the existence of the subsets $F_j$ such that
$\sum_{k\in F_j} a_{j,k} \gt (A/A')A_j$ when
$\sum_{j\in G} A_j = A' \gt A$. But this last equation means that
$A/A' \lt 1$, so $(A/A')A_j \lt A_j$. The first equation is therefore
just a consequence of the fact that $A_j$ is a least upper bound.</p>

<p>Next post, the warmups.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 1 Exam Problems]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/05/concrete-mathematics-chapter-1-exam-problems/"/>
    <updated>2012-02-05T12:27:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/05/concrete-mathematics-chapter-1-exam-problems</id>
    <content type="html"><![CDATA[<p>It took me longer than I thought, and the outcome is slightly
disappointing: I failed to solve two of the problems, and I solved the
remaining ones way too slowly, so in a real exam conditions I probably
would have solved just one or two...</p>

<!-- more -->


<h2>Exam Problems</h2>

<h3>4 Pegs Tower of Hanoi</h3>

<p>First, it helps to see that the indices of the recurrence are actually
$S_n$:</p>

<div markdown="0">
\begin{align}
W_{n(n+1)/2}&amp;= W_{S_n}\\\\
W_{n(n-1)/2}&amp;= W_{S_{n-1}}
\end{align}
</div>


<p>And of course, $S_n = S_{n-1} + n$.</p>

<p>Setting $m=S_{n-1}$, we try to show:</p>

<div markdown="0">
\begin{align}
W_{m+n} &amp; \le 2W_{m} + T_n\\\\
\end{align}
</div>


<p>Now, obviously, if we have $m+n$ discs, we can move the $m$ top ones
from $A$ to $C$ using $B$ and $D$ as transfer pegs, then move the
bottom $n$ ones from $A$ to $B$ using $D$ as transfer peg, and finally
move the top $m$ ones from $C$ to $B$.</p>

<p>The first step takes $W_m$ moves, the second one is the classic Tower
of Hanoi problem (as we can no longer use peg $C$, we only have three
pegs), so it takes $T_n$ moves, and the last step takes $W_m$  moves again.</p>

<p>This is only one possible solution; the optimal one must be equal or
better, so we have</p>

<div markdown="0">
\begin{align}
W_{m+n} &amp; \le 2W_m + T_n\\\\
\end{align}
</div>


<p>This is true for any $m+n$ discs, and in particular for
$S_n = S_{n-1} + n$ ones.</p>

<h3>Specific Zigs</h3>

<p>I could not solve this problem. I had found that the half-lines did
intersect, but then I failed to show that their intersections were all
distinct.</p>

<p>Even with the solution from the book, it took me a while before I
finally had a complete understanding.</p>

<p>One problem I had was that lines in a graph are basic college level
mathematics, but college was a long, long time ago. I pretty much had
to work from first principles.</p>

<p>Following the book in writing the positions as $(x_j, 0)$ and
$(x_j - a_j, 1)$, I need to find $\alpha$ and $\beta$ such that
$y=\alpha x + \beta$ is true for both points above.</p>

<div markdown="0">
\begin{align}
0 &amp; = \alpha x_j + \beta \\\\
\beta &amp; = - \alpha x_j\\\\
1 &amp; = \alpha (x_j - a_j) - \alpha x_j\\\\
&amp; = \alpha x_j - \alpha a_j - \alpha x_j\\\\
&amp; = - \alpha a_j\\\\
\alpha &amp; = \frac{-1}{a_j}\\\\
y &amp; = \frac{x_j - x}{a_j}\\\\
\end{align}
</div>


<p>With this given, I can try to find the intersection of lines from
different zigs, $j$ and $k$:</p>

<div markdown="0">
\begin{align}
\frac{x_j - x}{a_j} &amp; = \frac{x_k - x}{a_k}\\\\
a_k (x_j - x) &amp; = a_j (x_k - x)\\\\
a_k x_j - a_k x &amp; = a_j x_k - a_j x\\\\
a_k x_j - a_j x_k &amp; = (a_k - a_j) x\\\\
\end{align}
</div>


<p>Now, still following the book, I replace $x$ by $t$ with
$x=x_j - t a_j$:</p>

<div markdown="0">
\begin{align}
a_k x_j - a_j x_k &amp; = (a_k - a_j) (x_j - t a_j)\\\\
a_k x_j - a_j x_k &amp; = a_k x_j - a_j x_j - t a_j a_k + t a_j^2\\\\
- a_j x_k &amp; = t a_j^ 2 - a_j x_j - t a_j a_k\\\\
- x_k &amp; = t a_j - x_j -t a_k&amp;&amp;\text{dividing by \(a_j\)}\\\\
x_j - x_k &amp; = t (a_j - a_k)\\\\
t &amp; = \frac{x_j - x_k}{a_j - a_k}\\\\
\end{align}
</div>


<p>Somehow, I have a faint memory of such a result; I need to check a
college math book.</p>

<p>To complete, I need to show that $y = t$:</p>

<div markdown="0">
\begin{align}
y &amp; = \frac{x_j - x}{a_j}\\\\
&amp; = \frac{x_j - x_j + t a_j}{a_j}\\\\
&amp; = \frac{t a_j}{a_j}\\\\
&amp; = t\\\\
\end{align}
</div>


<p>So the intersection of any two pair of half-lines from different zigs
is $(x_j - t a_j, t)$. Note that $t$ has the same value whether
$j \gt k$ or $k \gt j$. To simplify further computations, I set
$j \gt k$.</p>

<p>There are two remaining steps: show that $t$ is different for
different pairs of $j$, $k$ (with $j \ne k$); and then show that the
four intersections for a pair $j$, $k$ are also distinct.</p>

<p>$a_j$ can be of two forms: $n^j$ and $n^j + n^{-n}$. So $a_j - a_k$
can be one of</p>

<div markdown="0">
\begin{align}
&amp; n^j - n^k\\\\
&amp; n^j + n^{-n} - n^k\\\\
&amp; n^j - n^k - n^{-n}\\\\
n^j + n^{-n} - n^k - n^{-n} = &amp; n^j - n^k\\\\
\end{align}
</div>


<p>So there are three different forms for $a_j - a_k$, which I will
simply write $n^j - n^k + \epsilon$ where $|\epsilon| \lt 1$.</p>

<div markdown="0">
\begin{align}
t &amp; = \frac{n^{2j} - n^{2k}}{n^j - n^k + \epsilon}\\\\
&amp; = \frac{(n^j - n^k)(n^j + n^k)}{n^j - n^k + \epsilon}\\\\
\end{align}
</div>


<p>Let's show that $n^j+n^k - 1 \lt t \lt n^j+n^k + 1$: multiply the
whole inequality by $n^j - n^k + \epsilon$. As</p>

<div markdown"0">
\begin{align}
n^j - n^k &amp; \ge n\\\\
&amp; \ge 2\\\\
&amp; \gt |\epsilon|\\\\
\end{align}
</div>


<p>so $n^j - n^k + \epsilon \gt 0$. Defining</p>

<div markdown="0">
\begin{align}
N_{jk} &amp; = n^j + n^k\\\\
N'_{jk} &amp; = n^j - n^k\\\\
\end{align}
</div>


<p>the left and right inequalities become</p>

<div markdown="0">
\begin{align}
(N_{jk} - 1) (N'_{jk} + \epsilon) &amp; = N_{jk}N'_{jk} - N'_{jk} + \epsilon N_{jk} - \epsilon\\\\
(N_{jk} + 1) (N'_{jk} + \epsilon) &amp; = N_{jk}N'_{jk} + N'_{jk} + \epsilon N_{jk} + \epsilon\\\\
\end{align}
</div>


<p>Subtracting $N_{jk}N'_{jk} = (n^j-n^k)(n^j+n^k)$ from the original inequality:</p>

<div markdown="0">
\begin{align}
-N'_{jk}+\epsilon N_jk - \epsilon \lt 0 \lt N'_{jk} + \epsilon N_{jk} + \epsilon\\\\
\end{align}
</div>


<p>I need to prove the following inequality</p>

<div markdown"0">
\begin{align}
(n^j - n^k) &amp; \gt |\epsilon| + |\epsilon| (n^j - n^k)\\\\
\end{align}
</div>


<p>We already know $|\epsilon| \lt 1$, so looking at the second term (and
assuming $\epsilon \ne 0$, as this case is trivial)</p>

<div markdown"0">
\begin{align}
|\epsilon| (n^j-n^k) &amp; = n^{-n} (n^j - n^k)\\\\
&amp; = n^{j-n} - n^{k-n}\\\\
&amp;\lt 1\\\\
\end{align}
</div>


<p>and we have</p>

<div markdown"0">
\begin{align}
n^j - n^k &amp; \ge 2
&amp; \gt |\epsilon| + |\epsilon (n^j - n^k)|\\\\
\end{align}
</div>


<p>So the inequalities are established. $N_{jk}$ can be seen as a number
in based $n$ where the digits are all zeroes except the $j$ and $k$ ones,
$N_{jk} = N_{j'k'} \implies j=j', k=k'$, and therefore $t$ uniquely
defines $j$ and $k$ or, two pairs of zigs must have different $t$.</p>

<p>I still need to show that for a given pair, when $t$ is the same, the
intersections are different. There are three different values of
$t$, so two intersections points have the same height. This happens
for</p>

<div markdown="0">
\begin{align}
t &amp; = \frac{n^{2j} - n^{2k}}{n^j - n^k}\\\\
\end{align}
</div>


<p>which happens when $a_j = n^j$, $a_k = n^k$ and $a_j = n^j + n^{-n}$,
$a_k = n^k + n^{-n}$. But the $x = x_j - t a_j$ value for
intersections is different: $t n^j$ and $t (n^j + n^{-n})$, so there
are indeed four distinct intersection points.</p>

<h3>30 degrees Zigs</h3>

<p>I could not solve this problem. Once again, my lack of intuition with
geometry was to blame.</p>

<p>But if we have two zigs with half-lines angles $\phi$, $\phi + 30^{\circ}$
and $\theta$, $\theta + 30^{\circ}$, then for any two pairs of
half-lines from the two zigs to intersect, their angles must be
between $0^{\circ}$ and $180^{\circ}$. Taken together, these
constraints give $0^{\circ} \lt |\phi - \theta| \lt 150^{\circ}$.</p>

<p>This means there cannot be more than $5$ such pairs (and to be honest,
I would have said 4, but the book says it's indeed 5).</p>

<h3>Recurrence Equations</h3>

<h3>Good and Bad Persons in Josephus Problem</h3>

<p>It took me a while, as I was trying to find a recurrence equation of
some sort which would help me with this problem and the bonus one
(where Josephus' position is fixed but he can pick $m$). Eventually I
found one, which did not help me with the bonus problem, but led me to
a solution for this problem.</p>

<p>Obviously, if we have $k$ persons and want to remove the last one in
the first round, we can choose $m=k$ and that will work. Actually, any
multiple $m=ak$ works as well.</p>

<p>This shows that at each round, if we have $k$ persons left, and we
start counting on the first one, when $m=ak$ we will remove the $k^{th}$
person then start counting from the first one again.</p>

<p>Back to the original problem: there are $2n$ persons, and we want to
get rid of the $n+1, \cdots, 2n$ first. If we take
$m=lcm(n+1,\cdots, 2n)$, then for the first $n$ rounds the last (bad)
person will be remove, leaving only the good ones at the end.</p>

<p>When first solving the problem, I picked $m=\prod_{i=1}^n (n+i)$,
which has the same property as the least common multiple, but is
larger. Perhaps a smaller number is better for the nerves of the
participants.</p>

<h3>Bonus Problems</h3>

<p>I tried to solve the bonus questions, but after repeatedly failing, I
had a glimpse at the solutions: they obviously require either
knowledge of later chapters, or other concepts I know nothing about,
so I will get back to these bonus problems after I finish the book.</p>

<p>I am now working through Chapter 2. It is a much larger chapter than
the first, so it will take me some time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Repertoire Method]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-repertoire-method/"/>
    <updated>2012-01-14T13:33:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-repertoire-method</id>
    <content type="html"><![CDATA[<p>The repertoire method is never really explained in the book, or
anywhere else I could find on the Internet. There are a couple of
posts on this subject, so I though I should add mine.</p>

<p>The repertoire method is really a tool to help with the intuitive step
of figuring out a closed formula for a recurrence equation. It does so
by breaking the original problem into smaller parts, with the hope
they might be easier to solve.</p>

<!-- more -->


<h3>Why it works</h3>

<p>Let's assume we have a system of recurrence equations with parameters,
so that the unknown function can be expressed as a linear combination
of other (unknown) functions where the coefficients are the parameters:</p>

<div markdown="0">
\begin{align}
g(1) &amp; = b(0, \alpha_1, \cdots, \alpha_m)\\\\
g(n) &amp; = r_n(g_1, \cdots, g_{n-1}, \alpha_1, \cdots, \alpha_m)\\\\
&amp; = \sum_{i=1}^m A_i(n)\alpha_i,
\end{align}
</div>


<p>We can consider $g$ as a specific point in a $m$-dimensional function
space (determined by both the recurrence equations, and the
parameters), and because $g$ is a linear combination, we can try to
find $m$ base functions (hopefully known or easy to compute)
$f_k(n) = \sum_{i=1}^m A_i(n)\alpha_{i_k}$ with $1 \le k \le m$, expressed in
terms of $m$ linearly independent vectors
$(\alpha_{1_k},\cdots,\alpha_{m_k})$.</p>

<p>In other words, if we can find $m$ linearly independent parameter
vectors such that, for each, we have a known solution $f_k(n)$, then
we can express the function $g$ as a linear combination of $f_k(n)$
for any parameters (because the $m$ $f_k(n)$ form a base for the
$m$-dimensional function space defined by the recurrence equations).</p>

<h3>How it works</h3>

<p>First, we need to check that the recurrence equations accept a
solution expressed as</p>

<div markdown="0">
\begin{align}
g(n) &amp; = \sum_{i=1}^m A_i(n)\alpha_i
\end{align}
</div>


<p>It is enough to plug this definition into the recurrence equations,
and make sure the different parameters always remain in different
terms.</p>

<p>Then we can either solve $f(n) = \sum_{i=1}^m A_i(n)\alpha_i$ for
known $f(n)$, or for known $\alpha_i$
parameters, as long as we end up with $m$ linearly independent
parameter vectors (or, as it is equivalent, $m$ linearly independent
known functions for specific parameters).</p>

<p>It is important to keep in mind that a solution can be searched from
both direction: either set a function and try to solve for the
parameters, or set the parameters and solve for the function.</p>

<h3>Homework exercise</h3>

<p>Given</p>

<div markdown="0">
\begin{align}
g(1) &amp; = \alpha\\\\
g(2n+j) &amp; = 3g(n) + \gamma n + \beta_j&amp;&amp;\text{for \(j=0, 1\) and \(n \gt 1 \)}\\\\
\end{align}
</div>


<p>We need to check that $g$ can be written as</p>

<div markdown="0">
\begin{align}
g(n) &amp; = \alpha A(n) + \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)\\\\
\end{align}
</div>


<p>The base case is trivial. The recurrence case is</p>

<div markdown="0">
\begin{align}
g(2n) &amp; = 3g(n) + \gamma n + \beta_0\\\\
&amp; = 3(\alpha A(n) +  \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)) + \gamma n \beta_0\\\\
&amp; = \alpha 3A(n) + \beta_0 (3 B_0(n) + 1) + \beta_1 3B_1(n) + \gamma (3C(n) + n)\\\\
g(2n+1) &amp; = 3g(n) + \gamma n + \beta_1\\\\
&amp; = 3(\alpha A(n) +  \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)) + + \gamma n\beta_1\\\\
&amp; = \alpha 3A(n) + \beta_0 3 B_0(n)+ \beta_1 (3B_1(n) + 1) + \gamma (3C(n) + n)\\\\
\end{align}
</div>


<p>so $g$ can be expressed as a linear combination of other functions,
with the parameters as the coefficients.</p>

<p>Now, when I tried to solve this problem, I didn't know I could set the
parameters to values that would lead to an easy solution ($\gamma = 0$
turns the problem into an easy to solve generalised radix-based
Josephus problem); instead I wasted a lot of time trying to find known
functions and solve for the parameters, which is why I have four steps
below instead of just two as in the book.</p>

<h4>$g(n) = n$</h4>

<p>As the book suggests, I tried to solve for $g(n) = n$:</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp;\alpha = 1\\\\
2n = g(2n) &amp; = 3g(n) + \gamma n + \beta_0\\\\
&amp; = 3n + \gamma n + \beta_0&amp;&amp;\gamma = -1, \beta_0 = 0\\\\
2n+1 = g(2n+1) &amp; = 3g(n) + \gamma n + \beta_1\\\\
&amp; = 3n - n + \beta_1&amp;&amp; \beta_1 = 1\\\\
\end{align}
</div>


<h4>$g(2^m+l) = 3^m$</h4>

<p>As the recurrence equation looks like the generalised radix-based
Josephus equation, I tried to solve for $g(2^m+1) = 3^m$:</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp;\alpha = 1\\\\
3^m = g(2^m+2l) &amp; = 3g(2^{m-1}+l) + \gamma (2^{m-1} + l) + \beta_0\\\\
&amp; = 3\cdot 3^{m-1} + \gamma (2^{m-1} + l) + \beta_0&amp;&amp; \beta_0, \gamma = 0\\\\
3^m = g(2^m+2l+1) &amp; = 3g(2^{m^1}+l) + \gamma (2^{m-1} + l) + \beta_1\\\\
&amp; = 3\cdot 3^{m-1}&amp;&amp;\beta_1 = 0\\\\
\end{align}
</div>


<h4>$g(n) = 1$</h4>

<p>I tried to solve for $g(n) = 1$, as it seemed useful to solve for a
constant (no linear combination of linearly independent non-constant
functions can produce a constant function).</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp; \alpha = 1\\\\
1 = g(2n+j) &amp; = 3g(n) + \gamma n + \beta_j\\\\
&amp; = 3 + \gamma n + \beta_j&amp;&amp; \gamma = 0, \beta_j = -2\\\\
\end{align}
</div>


<h4>$\alpha, \beta_1 = 1, \beta_0,  \gamma = 0$</h4>

<p>This is the step that took me the longest, and when I finally
understood I could fix the parameters, I was able to use the
radix-based Josephus solution.</p>

<p>The recurrence equations</p>

<div markdown="0">
\begin{align}
g(1) &amp; = 1\\\\
g(2n) &amp; = 3g(n)\\\\
g(2n+1) &amp; = 3g(n) + 1\\\\
\end{align}
</div>


<p>have as solution $g(2^m + (b_m\cdots b_0)) = 3^m + (b_m\cdots b_0)_3$.</p>

<h4>Solving for $g(n)$</h4>

<p>We have the equations</p>

<div markdown="0">
\begin{align}
A(n) - C(n) &amp; = n\\\\
A(2^m + l) &amp; = 3^m\\\\
A(n) -2(B_0(n) + B_1(n)) &amp; = 1\\\\
B_1(2^m+l) &amp; = h_3(l)&amp;&amp;\text{where \(h_3(b_m\cdots b_0) = (b_m\cdots b_0)_3\)}\\\\
\end{align}
</div>


<p>We have two functions already defined ($A(n)$ and $B_1(n)$), and the
other two equations give us the remaining function.</p>

<p>Now we can solve for $g(n)$:</p>

<div markdown="0">
\begin{align}
g(2^m+l) = \alpha 3^m &amp; + \beta_0 (\frac{3^m - 1}{2} - h_3(l))\\\\
&amp; + \beta_1 h_3(l) \\\\
&amp;+ \gamma (3^m + h_3(l) - 2^m - l)
\end{align}
</div>


<p>The $\gamma$ term is really $h_3(n) - n$.</p>

<p>The $\beta_0$ term is the same as $h_3(2^m-1-l)$, as can be seen by
observing that in base $3$, $3^m$ is $1$ followed by $m$ zeroes, so
$3^m-1$ is $m$ twos, and $\frac{3^m-1}{2}$ is $m$ ones, in other words
the same representation as the binary representation of $2^m-1$.</p>

<p>Now, the binary representation of $l$ is the same as the
representation in base $3$ of $h_3(l)$ (by definition of $h_3$), so
the binary representation of $2^m-1-l$ is the same as the
representation in base $3$ of $\frac{3^m-1}{2} - h_3(l)$.</p>

<p>With these two observations, it is possible to rewrite $g$ as</p>

<div markdown="0">
\begin{align}
g(1b_m\cdots b_0) &amp; = (\alpha\beta_{b_m}\cdots\beta_{b_0})_3 + \gamma ((1b_m\cdots b_0)_3 - (1b_m\cdots b_0)_2)
\end{align}
</div>


<p>which is the book solution.</p>

<h3>Faster solution</h3>

<p>It is enough to solve for
$\alpha, \beta_0, \beta_1 \ne 0, \gamma = 0$,
and to find the parameters for $g(n) = n$. The first gives $A$,
$B_0$ and $B_1$ directly by the generalised radix-based Josephus
solution, and the second one adds a constraint to solve for $C$ as well.</p>

<h3>Wrapping up</h3>

<p>As can be seen above, approaching the problem from both directions
(solving for known functions and solving for known parameters) can
result in time saved, and simplified expression of the solution.</p>
]]></content>
  </entry>
  
</feed>
