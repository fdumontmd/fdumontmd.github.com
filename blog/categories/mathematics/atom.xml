<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Mathematics | Wakatta!]]></title>
  <link href="http://blog.wakatta.jp/blog/categories/mathematics/atom.xml" rel="self"/>
  <link href="http://blog.wakatta.jp/"/>
  <updated>2012-02-28T22:39:10+09:00</updated>
  <id>http://blog.wakatta.jp/</id>
  <author>
    <name><![CDATA[Frédéric Dumont]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 2 Warmups]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/28/concrete-mathematics-chapter-2-warmups/"/>
    <updated>2012-02-28T19:18:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/28/concrete-mathematics-chapter-2-warmups</id>
    <content type="html"><![CDATA[<!--more-->


<h2>Warmups</h2>

<h3>$\sum_{k=4}^0 q_k$</h3>

<p>The meaning of such an expression is not clear, so there is no real
way to fail this exercise.</p>

<p>A first interpretation, maybe the common one, is that the sum is zero
because the range is empty. In other words, the sum is
$\sum_{4\le k\le 0} q_k$.</p>

<p>A second interpretation, perhaps for those used to programming
languages with very flexible loops could argue that the sum is
$q_4 + q_3 + q_2 + q_1 + q_0$.</p>

<p>I toyed briefly with a negative sum, similar to integrals with
reversed bounds, but I did not come up with the nice book solution
of $\sum_{k=m}^n = \sum_{k\le n} - \sum_{k\lt m}$, which is consistent
with and extends the first interpretation.</p>

<h3>Simplify $x([x\gt 0] - [x\lt 0])$</h3>

<p>It is easy to see that the expression has the same value as $|x|$:</p>

<div markdown="0">
\begin{align}
x([x\gt 0] - [x\lt 0]) &amp; = x (1-0)&amp;&amp;\text{when \(x\gt 0\)}\\\\
&amp; = x\\\\
x([x\gt 0] - [x\lt 0]) &amp; = x (0-1)&amp;&amp;\text{when \(x\lt 0\)}\\\\
&amp; = -x\\\\
x([x\gt 0] - [x\lt 0]) &amp; = 0&amp;&amp;\text{when \(x = 0\)}\\\\
\end{align}
</div>


<h3>Writing out sums</h3>

<p>The first one is easy:</p>

<div markdown="0">
\begin{align}
\sum_{0\le k\le 5}a_k = a_0+a_1+a_2+a_3+a_4+a_5\\\\
\end{align}
</div>


<p>The second one is tricky, is more than one way. One problem is that
$k$ is not explicitly defined, and I had assumed it was a natural,
when the authors thought of it as a integer; now the latter is in line
with the book conventions, so I was wrong and had missing terms. The
right answer is:</p>

<div markdown="0">
\begin{align}
\sum_{0\le k^2 \le 5}a_k = a_4 + a_1 + a_0 + a_1 + a_4\\\\
\end{align}
</div>


<h3>Triple Sum</h3>

<p>Here it is important to restrict the bounds as much as possible (but
no more); otherwise there is a risk of introducing spurious terms.</p>

<div markdown="0">
\begin{align}
\sum_{1\le i \lt j \lt k \le n}a_{ijk} &amp; = \sum_{i=1}^2 \sum_{j=i+1}^3 \sum_{k=j+1}^4 a_{ijk}\\\\
&amp; = \left((a_{123} + a_{124}) + a_{134} \right) + a_{234}\\\\
&amp; = \sum_{k=3}^4 \sum_{j=2}^{k-1} \sum_{i=1}^{j-1} a_{ijk}\\\\
&amp; = a_{123}+\left(a_{124} + (a_{134} + a_{234})\right)\\\\
\end{align}
</div>


<p>The terms appear in the same order, but are grouped in sums differently.</p>

<h3>Incorrect derivation</h3>

<p>The problem is the step</p>

<div markdown="0">
\begin{align}
\sum_{j=1}^n \sum_{k=1}^n = \frac{a_j}{a_k}\sum_{k=1}^n \sum_{k=1}^n \frac{a_k}{a_k}\\\\
\end{align}
</div>


<p>$k$ is already bound in the inner sum, so it is invalid to replace $j$
by $k$ in the outer.</p>

<h3>$\sum_k [1\le j\le k\le n]$</h3>

<p>This can be worked explicitly:</p>

<div markdown="0">
\begin{align}
\sum_k [1 \le j \le k \le n] &amp = \sum_k [1 \le j \le n] [j \le k \le n]\\\\
&amp; = \sum_{j\le k \le n} [1 \le j \le n]\\\\
&amp; = [1 \le j \le n] \sum_{j\le k \le n} 1\\\\
&amp; = [1 \le j \le n] (n-j+1)\\\\
\end{align}
</div>


<h3>$\bigtriangledown f(x)$</h3>

<p>The result is not surprising:</p>

<div markdown="0">
\begin{align}
\bigtriangledown x^{\overline{m}} &amp; = x^{\overline{m}} - (x-1)^{\overline{m}}\\\\
&amp; = x(x+1)\cdots(x+m-1) - (x-1)x\cdots(x+m-2)\\\\
&amp; = x(x+1)\cdots(x+m-2)(x+m-1-(x-1))\\\\
&amp; = m x^{\overline{m-1}}\\\\
\end{align}
</div>


<p>So $\bigtriangledown f(x)$ is the difference operator to use with
rising factorials.</p>

<h3>$0^{\overline{m}}$</h3>

<p>Clearly, when $m\lt 0$, $0^{\overline{m}} = 0$; when $m = 0$,
$0^{\overline{m}} = 1$ (to make the expression
$x^{\underline{1+0}}=x^{\underline 1}(x-1)^{\underline 0}$ work when $x=1$); I
had forgotten about $m&lt;0$, which was perhaps the easiest case, as $\frac{1}{m!}$
(it follows directly from the definition of falling factorials with negative
powers).</p>

<h3>Law of exponents for rising factorials</h3>

<p>It is easy to see that $x^{\overline{m+n}} = x^{\overline m}(x+m)^{\overline n}$:</p>

<div markdown="0">
\begin{align}
x^{\overline{m+n}} &amp; = x\cdots(x+m-1)(x+m)\cdots(x+m+n-1)\\\\
&amp; = \left( x\cdots(x+m-1) \right) \left( (x+m)\cdots(x+m+n-1) \right)\\\\
&amp; = x^{\overline m}(x+m)^{\overline n}\\\\
\end{align}
</div>


<p>From there, the value of rising factorials for negative powers follows quickly:</p>

<div markdown="0">
\begin{align}
1 = x^{\overline{-n+n}} &amp; = x^{\overline{-n}} (x-n)^\overline{n}\\\\
x^{\overline{-1}} &amp; = \frac{1}{(x-n)^\overline{n}}\\\\
&amp; = \frac{1}{(x-n)\cdots(x-1)}\\\\
&amp; = \frac{1}{(x-1)^{\underline{n}}}\\\\
\end{align}
</div>


<h3>Symmetric difference of a product</h3>

<p>To start, I quickly looked up the proof of the original derivative
product rule on
<a href="http://en.wikipedia.org/wiki/Product_rule#Proof_of_the_product_rule">Wikipedia</a>;
the geometric nature of the proof was illuminating (I believe I was
taught the so called
<a href="http://en.wikipedia.org/wiki/Product_rule#A_Brief_Proof">Brief Proof</a>
both in high-school and at university).</p>

<p>This geometric proof can be used for both the infinite and the finite
calculus, and its symmetric nature (there are two ways to compute the
area of the big rectangle:
$f(x)g(x)+(f(w)-f(x))g(w) + f(x)(g(w)-g(x))$ and
$f(x)g(x)+f(w)(g(w)-g(x)) + (f(w)-f(x))g(x)$) can be used in the
finite case. The symmetry (and equality) is restored
because in the infinite calculus, $\lim_{w\rightarrow x}f(w) = f(x)$
and $\lim_{w\rightarrow x}g(w) = g(x)$, a restoration that is not
possible in the finite calculus.</p>

<p>However, the equivalent finite calculus formulas,
$\bigtriangleup(uv) = u\bigtriangleup v + Ev\bigtriangleup u$ and
$\bigtriangleup(uv) = Eu\bigtriangleup v + v\bigtriangleup u$, have
together the symmetry they lack on their own.</p>

<h3>Wrapping up</h3>

<p>OK, that was not entirely bad (two small mistakes, both about negative
numbers blindness). Next step, the basic exercises.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 2 Notes]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/27/concrete-mathematics-chapter-2-notes/"/>
    <updated>2012-02-27T10:54:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/27/concrete-mathematics-chapter-2-notes</id>
    <content type="html"><![CDATA[<p>After a long but busy silence, I have now a few notes on the second
chapter, Sums. As with
<a href="/blog/2012/01/06/concrete-mathematics-chapter-1-notes/">Chapter 1</a>,
these are nothing revolutionary; just some clarifications of the
points that were not obvious to me, as well as other, random
observations.</p>

<!--more-->


<p>Overall, this chapter felt less overwhelming than the first, despite
being much longer and introducing very powerful techniques. I have yet
to do the exercises, though, so I may still revise this judgement.</p>

<h3>Notation</h3>

<p>The authors mentions that the Sigma-notation is "... impressive to
family and friends". I can confirm that assessment.</p>

<p>The remark on keeping bounds simple actually goes beyond resisting
"premature optimisation", that is, removing terms just because they
are equal to zero. Sometimes, it is worth adding a zero term if it
simplifies the bounds. Such a trick is used in solving
$\sum_{1\le j\lt k\le n} \frac{1}{k-j}$, and I'll get back to this
point when I go over this solution.</p>

<p>The Iverson notation (or Iversonian) is a very useful tool, as is the
general Sigma-notation. About the latter, it already simplifies
variable changes a lot, but I found it useful (and less error prone)
to always write the variable change on the right margin (for instance
as $k \leftarrow k+1$) and to keep that change as the only one in a
given line of the rewrite; otherwise, no matter how trivial the
change, any error I make at that time will be hard to locate (I know;
I tried).</p>

<h3>Sums and Recurrence</h3>

<p>First we see how easy it is to use the repertoire method to build
solutions to common (or slightly generalised) sums. The only problem
with the repertoire method is it requires a well furnished repertoire
of solutions to basic recurrences; I'm sure I would never have come up
with the radix-change solution to the generalised Josephus
problem. And given that there is an infinite number of functions one
could try, a more directed method is sometimes necessary.</p>

<p>This section also shows how to turn some recurrence equations (such as
the Tower of Hanoi one) into a sum; this method involve a choice
($s_1$ can be any non-zero value), which could either simplify or
complicate the solution. I haven't done the exercises yet, so I don't
know to what extent the choice is obvious or tricky.</p>

<p>Finally it shows how to turn a recurrence expressed as a sum of all
the previous values into a simpler recurrence by computing the
difference between two successive values. This is one instance of a
more general simplification using a linear combination of a few
successive values.</p>

<h3>Manipulation of Sums</h3>

<p>Unsurprisingly, sums have the same basic properties as common
additions: distributive, associative and commutative laws. Only the
latter is really tricky, as it involves a change to the index
variable. As mentioned above, I found useful to make such changes
really clear and isolated in any reasoning.</p>

<p>With these laws confirmed, it is possible to build the first method
for solving sums: the perturbation method. It is very simple, and
while it does not always work, when it does it is very quick.</p>

<h3>Multiple Sums</h3>

<p>This is perhaps the first section where I had to slow down; basically
multiple sums are not different from simple sums, and manipulations
are defined by the distributive law, but index variable changes
(especially the rocky road variety) require special attention. This,
combined with "obvious" simplifications (obvious to the authors, and
sometimes in retrospect to the reader as well), gave me some
difficulties.</p>

<p>For instance, the solution to</p>

<div markdown="0">
\begin{align}
\sum_{1\le j\lt k\le n} \frac{1}{k-j}
\end{align}
</div>


<p>The index variable change $k \leftarrow k+j$ is explained as a
specific instance of the simplification of $k+f(j)$; more perplexing
are the ranges for $j$ and $k$ when the sum is replaced by a sum of sum:</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\le n} \sum_{1\le j \le n-k} \frac{1}{k}
\end{align}
</div>


<p>The range for $j$ is built from $1\le j$ and $k+j\le n$, so there is
nothing really strange here.</p>

<p>The range for $k$, however, looks like a typo: certainly the authors
meant $1\le k\lt n$. A margin graffiti confirms the range, but it does
not really explain it.</p>

<p>The fact is, it is safe to let $k\le n$ here, because the sum over $j$
when $k=n$ is zero: not only the expression
$\sum_{1\le j \le k-n = 0} \frac{1}{k}$ is zero because there is no
$j$ that can satisfies the range predicate, but the closed form
of this sum, $\frac{k-n}{k}$, is also zero when $k=n$.</p>

<p>With the closed form checked, it is safe to add extra terms to
simplify the range of $k$.</p>

<p>What happens if you don't see this possible simplification? As
expected, the answer remains the same:</p>

<div markdown="0">
\begin{align}
\sum_{1\le k\lt n} \sum_{1\le j \le n-k} \frac{1}{k} &amp; = \sum_{1\le k\lt n} \frac{n-k}{k}\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} - \sum_{1\le k\lt n} \frac{k}{k}\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} - (n-1)\\\\
&amp; = \sum_{1\le k\lt n} \frac{n}{k} + \frac{n}{n} - n\\\\
&amp; = \sum_{1\le k\le n} \frac{n}{k} - n\\\\
&amp; = nH_n - n\\\\
\end{align}
</div>


<p>So to expend on the original advice of keeping the bounds as simple as
possible: sometimes it is possible to extend the bounds (in order to
simplify them), as long as the extra terms in closed form evaluate to
zero. If the extra terms are still defined as sums, just checking that
the range is empty might not be enough.</p>

<h3>General Methods</h3>

<p>A cool and fun section on the various ways to solve a given sum.</p>

<p>Method 0 is to look it up. This book, written before the rise of
Internet (I remember Internet in the early 1990's; most of it was still
indexed manually on the CERN index pages...), suggests a few books as
resources.</p>

<p>Fortunately, some of them have migrated to the
<a href="https://oeis.org/">Web</a>, which is a more suitable tool than books for
such knowledge; the combination of searches and instant updates is
hard to beat (a book remains best for a content that is mostly linear
and somewhat independent of time; a novel, or textbook, for
instance. References are better on Internet, free if possible, for
a subscription otherwise).</p>

<p>Method 1 is guessing then proving; proving in fact should be a
complement for all the other methods (except perhaps Method 0). Having
two independents proofs is always good.</p>

<p>Method 2 is the perturbation method. In this section example, we see
how an apparent failure can still be exploited by being imaginative.</p>

<p>Method 3 is the repertoire method. In this chapter it is usually much
simpler than in the first.</p>

<p>Method 4 uses calculus to get a first approximation, then uses other
methods to solve the equations for the error function.</p>

<p>Method 5 is a clever rewriting of the problem into a sum of sums;
like the repertoire method but unlike the others, it requires some
intuition to find a solution (perhaps more than the repertoire
method); I have bad memories of trying such a method to solve problems
at university, always somehow ending up right where I started. I guess
I will try other methods if I can.</p>

<p>Method 6 is the topic of the next section; method 7 is for another
chapter.</p>

<h3>Finite and Infinite Calculus</h3>

<p>This section was surprising and exciting, but not really that
complex. It really is a matter of adapting regular calculus reflexes to the
finite version. I have to see how it works in practice.</p>

<p>One thing that is causing me some trouble is the falling-power version
of the law of exponents:</p>

<div markdown="0">
\begin{align}
x^{\underline{m+n}} &amp; = x^{\underline m}(x-m)^{\underline n}\\\\
\end{align}
</div>


<p>While the rule is easy to prove and to remember, it is less easy than
the general one to recognise in practice; I failed to see it when it
came up in the solution to</p>

<div markdown="0">
\begin{align}
\Sigma xH_x\delta x\\\\
\end{align}
</div>


<p>Worse, even the explanation in the book, I had to write it down, play
with it, before seeing it.</p>

<p>So I'm thinking about a notation that would bring out the rule more
clearly, an extension of the <em>shift operator</em> $E$:</p>

<div markdown="0">
\begin{align}
E_k f(x) &amp; = f(x-k)\\\\
\end{align}
</div>


<p>This would turn the exponent law into</p>

<div markdown="0">
\begin{align}
x^{\underline{m+n}} &amp; = x^{\underline m} E_m x^{\underline n}\\\\
\end{align}
</div>


<p>Whether this is useful, or whether I'll get used to the original
notation anyway, we'll see in the exercises...</p>

<h3>Infinite Sums</h3>

<p>The last section is about infinite sums. The authors quite sensibly
restrict the scope to absolutely convergent sums, which have the
advantage that the three basic laws and the manipulations they allow
are still valid.</p>

<p>Once again, this was not overly difficult; the only point I had
trouble understanding was the existence of the subsets $F_j$ such that
$\sum_{k\in F_j} a_{j,k} \gt (A/A')A_j$ when
$\sum_{j\in G} A_j = A' \gt A$. But this last equation means that
$A/A' \lt 1$, so $(A/A')A_j \lt A_j$. The first equation is therefore
just a consequence of the fact that $A_j$ is a least upper bound.</p>

<p>Next post, the warmups.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 1 Exam Problems]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/02/05/concrete-mathematics-chapter-1-exam-problems/"/>
    <updated>2012-02-05T12:27:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/02/05/concrete-mathematics-chapter-1-exam-problems</id>
    <content type="html"><![CDATA[<p>It took me longer than I thought, and the outcome is slightly
disappointing: I failed to solve two of the problems, and I solved the
remaining ones way too slowly, so in a real exam conditions I probably
would have solved just one or two...</p>

<!-- more -->


<h2>Exam Problems</h2>

<h3>4 Pegs Tower of Hanoi</h3>

<p>First, it helps to see that the indices of the recurrence are actually
$S_n$:</p>

<div markdown="0">
\begin{align}
W_{n(n+1)/2}&amp;= W_{S_n}\\\\
W_{n(n-1)/2}&amp;= W_{S_{n-1}}
\end{align}
</div>


<p>And of course, $S_n = S_{n-1} + n$.</p>

<p>Setting $m=S_{n-1}$, we try to show:</p>

<div markdown="0">
\begin{align}
W_{m+n} &amp; \le 2W_{m} + T_n\\\\
\end{align}
</div>


<p>Now, obviously, if we have $m+n$ discs, we can move the $m$ top ones
from $A$ to $C$ using $B$ and $D$ as transfer pegs, then move the
bottom $n$ ones from $A$ to $B$ using $D$ as transfer peg, and finally
move the top $m$ ones from $C$ to $B$.</p>

<p>The first step takes $W_m$ moves, the second one is the classic Tower
of Hanoi problem (as we can no longer use peg $C$, we only have three
pegs), so it takes $T_n$ moves, and the last step takes $W_m$  moves again.</p>

<p>This is only one possible solution; the optimal one must be equal or
better, so we have</p>

<div markdown="0">
\begin{align}
W_{m+n} &amp; \le 2W_m + T_n\\\\
\end{align}
</div>


<p>This is true for any $m+n$ discs, and in particular for
$S_n = S_{n-1} + n$ ones.</p>

<h3>Specific Zigs</h3>

<p>I could not solve this problem. I had found that the half-lines did
intersect, but then I failed to show that their intersections were all
distinct.</p>

<p>Even with the solution from the book, it took me a while before I
finally had a complete understanding.</p>

<p>One problem I had was that lines in a graph are basic college level
mathematics, but college was a long, long time ago. I pretty much had
to work from first principles.</p>

<p>Following the book in writing the positions as $(x_j, 0)$ and
$(x_j - a_j, 1)$, I need to find $\alpha$ and $\beta$ such that
$y=\alpha x + \beta$ is true for both points above.</p>

<div markdown="0">
\begin{align}
0 &amp; = \alpha x_j + \beta \\\\
\beta &amp; = - \alpha x_j\\\\
1 &amp; = \alpha (x_j - a_j) - \alpha x_j\\\\
&amp; = \alpha x_j - \alpha a_j - \alpha x_j\\\\
&amp; = - \alpha a_j\\\\
\alpha &amp; = \frac{-1}{a_j}\\\\
y &amp; = \frac{x_j - x}{a_j}\\\\
\end{align}
</div>


<p>With this given, I can try to find the intersection of lines from
different zigs, $j$ and $k$:</p>

<div markdown="0">
\begin{align}
\frac{x_j - x}{a_j} &amp; = \frac{x_k - x}{a_k}\\\\
a_k (x_j - x) &amp; = a_j (x_k - x)\\\\
a_k x_j - a_k x &amp; = a_j x_k - a_j x\\\\
a_k x_j - a_j x_k &amp; = (a_k - a_j) x\\\\
\end{align}
</div>


<p>Now, still following the book, I replace $x$ by $t$ with
$x=x_j - t a_j$:</p>

<div markdown="0">
\begin{align}
a_k x_j - a_j x_k &amp; = (a_k - a_j) (x_j - t a_j)\\\\
a_k x_j - a_j x_k &amp; = a_k x_j - a_j x_j - t a_j a_k + t a_j^2\\\\
- a_j x_k &amp; = t a_j^ 2 - a_j x_j - t a_j a_k\\\\
- x_k &amp; = t a_j - x_j -t a_k&amp;&amp;\text{dividing by \(a_j\)}\\\\
x_j - x_k &amp; = t (a_j - a_k)\\\\
t &amp; = \frac{x_j - x_k}{a_j - a_k}\\\\
\end{align}
</div>


<p>Somehow, I have a faint memory of such a result; I need to check a
college math book.</p>

<p>To complete, I need to show that $y = t$:</p>

<div markdown="0">
\begin{align}
y &amp; = \frac{x_j - x}{a_j}\\\\
&amp; = \frac{x_j - x_j + t a_j}{a_j}\\\\
&amp; = \frac{t a_j}{a_j}\\\\
&amp; = t\\\\
\end{align}
</div>


<p>So the intersection of any two pair of half-lines from different zigs
is $(x_j - t a_j, t)$. Note that $t$ has the same value whether
$j \gt k$ or $k \gt j$. To simplify further computations, I set
$j \gt k$.</p>

<p>There are two remaining steps: show that $t$ is different for
different pairs of $j$, $k$ (with $j \ne k$); and then show that the
four intersections for a pair $j$, $k$ are also distinct.</p>

<p>$a_j$ can be of two forms: $n^j$ and $n^j + n^{-n}$. So $a_j - a_k$
can be one of</p>

<div markdown="0">
\begin{align}
&amp; n^j - n^k\\\\
&amp; n^j + n^{-n} - n^k\\\\
&amp; n^j - n^k - n^{-n}\\\\
n^j + n^{-n} - n^k - n^{-n} = &amp; n^j - n^k\\\\
\end{align}
</div>


<p>So there are three different forms for $a_j - a_k$, which I will
simply write $n^j - n^k + \epsilon$ where $|\epsilon| \lt 1$.</p>

<div markdown="0">
\begin{align}
t &amp; = \frac{n^{2j} - n^{2k}}{n^j - n^k + \epsilon}\\\\
&amp; = \frac{(n^j - n^k)(n^j + n^k)}{n^j - n^k + \epsilon}\\\\
\end{align}
</div>


<p>Let's show that $n^j+n^k - 1 \lt t \lt n^j+n^k + 1$: multiply the
whole inequality by $n^j - n^k + \epsilon$. As</p>

<div markdown"0">
\begin{align}
n^j - n^k &amp; \ge n\\\\
&amp; \ge 2\\\\
&amp; \gt |\epsilon|\\\\
\end{align}
</div>


<p>so $n^j - n^k + \epsilon \gt 0$. Defining</p>

<div markdown="0">
\begin{align}
N_{jk} &amp; = n^j + n^k\\\\
N'_{jk} &amp; = n^j - n^k\\\\
\end{align}
</div>


<p>the left and right inequalities become</p>

<div markdown="0">
\begin{align}
(N_{jk} - 1) (N'_{jk} + \epsilon) &amp; = N_{jk}N'_{jk} - N'_{jk} + \epsilon N_{jk} - \epsilon\\\\
(N_{jk} + 1) (N'_{jk} + \epsilon) &amp; = N_{jk}N'_{jk} + N'_{jk} + \epsilon N_{jk} + \epsilon\\\\
\end{align}
</div>


<p>Subtracting $N_{jk}N'_{jk} = (n^j-n^k)(n^j+n^k)$ from the original inequality:</p>

<div markdown="0">
\begin{align}
-N'_{jk}+\epsilon N_jk - \epsilon \lt 0 \lt N'_{jk} + \epsilon N_{jk} + \epsilon\\\\
\end{align}
</div>


<p>I need to prove the following inequality</p>

<div markdown"0">
\begin{align}
(n^j - n^k) &amp; \gt |\epsilon| + |\epsilon| (n^j - n^k)\\\\
\end{align}
</div>


<p>We already know $|\epsilon| \lt 1$, so looking at the second term (and
assuming $\epsilon \ne 0$, as this case is trivial)</p>

<div markdown"0">
\begin{align}
|\epsilon| (n^j-n^k) &amp; = n^{-n} (n^j - n^k)\\\\
&amp; = n^{j-n} - n^{k-n}\\\\
&amp;\lt 1\\\\
\end{align}
</div>


<p>and we have</p>

<div markdown"0">
\begin{align}
n^j - n^k &amp; \ge 2
&amp; \gt |\epsilon| + |\epsilon (n^j - n^k)|\\\\
\end{align}
</div>


<p>So the inequalities are established. $N_{jk}$ can be seen as a number
in based $n$ where the digits are all zeroes except the $j$ and $k$ ones,
$N_{jk} = N_{j'k'} \implies j=j', k=k'$, and therefore $t$ uniquely
defines $j$ and $k$ or, two pairs of zigs must have different $t$.</p>

<p>I still need to show that for a given pair, when $t$ is the same, the
intersections are different. There are three different values of
$t$, so two intersections points have the same height. This happens
for</p>

<div markdown="0">
\begin{align}
t &amp; = \frac{n^{2j} - n^{2k}}{n^j - n^k}\\\\
\end{align}
</div>


<p>which happens when $a_j = n^j$, $a_k = n^k$ and $a_j = n^j + n^{-n}$,
$a_k = n^k + n^{-n}$. But the $x = x_j - t a_j$ value for
intersections is different: $t n^j$ and $t (n^j + n^{-n})$, so there
are indeed four distinct intersection points.</p>

<h3>30 degrees Zigs</h3>

<p>I could not solve this problem. Once again, my lack of intuition with
geometry was to blame.</p>

<p>But if we have two zigs with half-lines angles $\phi$, $\phi + 30^{\circ}$
and $\theta$, $\theta + 30^{\circ}$, then for any two pairs of
half-lines from the two zigs to intersect, their angles must be
between $0^{\circ}$ and $180^{\circ}$. Taken together, these
constraints give $0^{\circ} \lt |\phi - \theta| \lt 150^{\circ}$.</p>

<p>This means there cannot be more than $5$ such pairs (and to be honest,
I would have said 4, but the book says it's indeed 5).</p>

<h3>Recurrence Equations</h3>

<h3>Good and Bad Persons in Josephus Problem</h3>

<p>It took me a while, as I was trying to find a recurrence equation of
some sort which would help me with this problem and the bonus one
(where Josephus' position is fixed but he can pick $m$). Eventually I
found one, which did not help me with the bonus problem, but led me to
a solution for this problem.</p>

<p>Obviously, if we have $k$ persons and want to remove the last one in
the first round, we can choose $m=k$ and that will work. Actually, any
multiple $m=ak$ works as well.</p>

<p>This shows that at each round, if we have $k$ persons left, and we
start counting on the first one, when $m=ak$ we will remove the $k^{th}$
person then start counting from the first one again.</p>

<p>Back to the original problem: there are $2n$ persons, and we want to
get rid of the $n+1, \cdots, 2n$ first. If we take
$m=lcm(n+1,\cdots, 2n)$, then for the first $n$ rounds the last (bad)
person will be remove, leaving only the good ones at the end.</p>

<p>When first solving the problem, I picked $m=\prod_{i=1}^n (n+i)$,
which has the same property as the least common multiple, but is
larger. Perhaps a smaller number is better for the nerves of the
participants.</p>

<h3>Bonus Problems</h3>

<p>I tried to solve the bonus questions, but after repeatedly failing, I
had a glimpse at the solutions: they obviously require either
knowledge of later chapters, or other concepts I know nothing about,
so I will get back to these bonus problems after I finish the book.</p>

<p>I am now working through Chapter 2. It is a much larger chapter than
the first, so it will take me some time.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Repertoire Method]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-repertoire-method/"/>
    <updated>2012-01-14T13:33:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-repertoire-method</id>
    <content type="html"><![CDATA[<p>The repertoire method is never really explained in the book, or
anywhere else I could find on the Internet. There are a couple of
posts on this subject, so I though I should add mine.</p>

<p>The repertoire method is really a tool to help with the intuitive step
of figuring out a closed formula for a recurrence equation. It does so
by breaking the original problem into smaller parts, with the hope
they might be easier to solve.</p>

<!-- more -->


<h3>Why it works</h3>

<p>Let's assume we have a system of recurrence equations with parameters,
so that the unknown function can be expressed as a linear combination
of other (unknown) functions where the coefficients are the parameters:</p>

<div markdown="0">
\begin{align}
g(1) &amp; = b(0, \alpha_1, \cdots, \alpha_m)\\\\
g(n) &amp; = r_n(g_1, \cdots, g_{n-1}, \alpha_1, \cdots, \alpha_m)\\\\
&amp; = \sum_{i=1}^m A_i(n)\alpha_i,
\end{align}
</div>


<p>We can consider $g$ as a specific point in a $m$-dimensional function
space (determined by both the recurrence equations, and the
parameters), and because $g$ is a linear combination, we can try to
find $m$ base functions (hopefully known or easy to compute)
$f_k(n) = \sum_{i=1}^m A_i(n)\alpha_{i_k}$ with $1 \le k \le m$, expressed in
terms of $m$ linearly independent vectors
$(\alpha_{1_k},\cdots,\alpha_{m_k})$.</p>

<p>In other words, if we can find $m$ linearly independent parameter
vectors such that, for each, we have a known solution $f_k(n)$, then
we can express the function $g$ as a linear combination of $f_k(n)$
for any parameters (because the $m$ $f_k(n)$ form a base for the
$m$-dimensional function space defined by the recurrence equations).</p>

<h3>How it works</h3>

<p>First, we need to check that the recurrence equations accept a
solution expressed as</p>

<div markdown="0">
\begin{align}
g(n) &amp; = \sum_{i=1}^m A_i(n)\alpha_i
\end{align}
</div>


<p>It is enough to plug this definition into the recurrence equations,
and make sure the different parameters always remain in different
terms.</p>

<p>Then we can either solve $f(n) = \sum_{i=1}^m A_i(n)\alpha_i$ for
known $f(n)$, or for known $\alpha_i$
parameters, as long as we end up with $m$ linearly independent
parameter vectors (or, as it is equivalent, $m$ linearly independent
known functions for specific parameters).</p>

<p>It is important to keep in mind that a solution can be searched from
both direction: either set a function and try to solve for the
parameters, or set the parameters and solve for the function.</p>

<h3>Homework exercise</h3>

<p>Given</p>

<div markdown="0">
\begin{align}
g(1) &amp; = \alpha\\\\
g(2n+j) &amp; = 3g(n) + \gamma n + \beta_j&amp;&amp;\text{for \(j=0, 1\) and \(n \gt 1 \)}\\\\
\end{align}
</div>


<p>We need to check that $g$ can be written as</p>

<div markdown="0">
\begin{align}
g(n) &amp; = \alpha A(n) + \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)\\\\
\end{align}
</div>


<p>The base case is trivial. The recurrence case is</p>

<div markdown="0">
\begin{align}
g(2n) &amp; = 3g(n) + \gamma n + \beta_0\\\\
&amp; = 3(\alpha A(n) +  \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)) + \gamma n \beta_0\\\\
&amp; = \alpha 3A(n) + \beta_0 (3 B_0(n) + 1) + \beta_1 3B_1(n) + \gamma (3C(n) + n)\\\\
g(2n+1) &amp; = 3g(n) + \gamma n + \beta_1\\\\
&amp; = 3(\alpha A(n) +  \beta_0 B_0(n) + \beta_1 B_1(n) + \gamma C(n)) + + \gamma n\beta_1\\\\
&amp; = \alpha 3A(n) + \beta_0 3 B_0(n)+ \beta_1 (3B_1(n) + 1) + \gamma (3C(n) + n)\\\\
\end{align}
</div>


<p>so $g$ can be expressed as a linear combination of other functions,
with the parameters as the coefficients.</p>

<p>Now, when I tried to solve this problem, I didn't know I could set the
parameters to values that would lead to an easy solution ($\gamma = 0$
turns the problem into an easy to solve generalised radix-based
Josephus problem); instead I wasted a lot of time trying to find known
functions and solve for the parameters, which is why I have four steps
below instead of just two as in the book.</p>

<h4>$g(n) = n$</h4>

<p>As the book suggests, I tried to solve for $g(n) = n$:</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp;\alpha = 1\\\\
2n = g(2n) &amp; = 3g(n) + \gamma n + \beta_0\\\\
&amp; = 3n + \gamma n + \beta_0&amp;&amp;\gamma = -1, \beta_0 = 0\\\\
2n+1 = g(2n+1) &amp; = 3g(n) + \gamma n + \beta_1\\\\
&amp; = 3n - n + \beta_1&amp;&amp; \beta_1 = 1\\\\
\end{align}
</div>


<h4>$g(2^m+l) = 3^m$</h4>

<p>As the recurrence equation looks like the generalised radix-based
Josephus equation, I tried to solve for $g(2^m+1) = 3^m$:</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp;\alpha = 1\\\\
3^m = g(2^m+2l) &amp; = 3g(2^{m-1}+l) + \gamma (2^{m-1} + l) + \beta_0\\\\
&amp; = 3\cdot 3^{m-1} + \gamma (2^{m-1} + l) + \beta_0&amp;&amp; \beta_0, \gamma = 0\\\\
3^m = g(2^m+2l+1) &amp; = 3g(2^{m^1}+l) + \gamma (2^{m-1} + l) + \beta_1\\\\
&amp; = 3\cdot 3^{m-1}&amp;&amp;\beta_1 = 0\\\\
\end{align}
</div>


<h4>$g(n) = 1$</h4>

<p>I tried to solve for $g(n) = 1$, as it seemed useful to solve for a
constant (no linear combination of linearly independent non-constant
functions can produce a constant function).</p>

<div markdown="0">
\begin{align}
1 = g(1) &amp; = \alpha&amp;&amp; \alpha = 1\\\\
1 = g(2n+j) &amp; = 3g(n) + \gamma n + \beta_j\\\\
&amp; = 3 + \gamma n + \beta_j&amp;&amp; \gamma = 0, \beta_j = -2\\\\
\end{align}
</div>


<h4>$\alpha, \beta_1 = 1, \beta_0,  \gamma = 0$</h4>

<p>This is the step that took me the longest, and when I finally
understood I could fix the parameters, I was able to use the
radix-based Josephus solution.</p>

<p>The recurrence equations</p>

<div markdown="0">
\begin{align}
g(1) &amp; = 1\\\\
g(2n) &amp; = 3g(n)\\\\
g(2n+1) &amp; = 3g(n) + 1\\\\
\end{align}
</div>


<p>have as solution $g(2^m + (b_m\cdots b_0)) = 3^m + (b_m\cdots b_0)_3$.</p>

<h4>Solving for $g(n)$</h4>

<p>We have the equations</p>

<div markdown="0">
\begin{align}
A(n) - C(n) &amp; = n\\\\
A(2^m + l) &amp; = 3^m\\\\
A(n) -2(B_0(n) + B_1(n)) &amp; = 1\\\\
B_1(2^m+l) &amp; = h_3(l)&amp;&amp;\text{where \(h_3(b_m\cdots b_0) = (b_m\cdots b_0)_3\)}\\\\
\end{align}
</div>


<p>We have two functions already defined ($A(n)$ and $B_1(n)$), and the
other two equations give us the remaining function.</p>

<p>Now we can solve for $g(n)$:</p>

<div markdown="0">
\begin{align}
g(2^m+l) = \alpha 3^m &amp; + \beta_0 (\frac{3^m - 1}{2} - h_3(l))\\\\
&amp; + \beta_1 h_3(l) \\\\
&amp;+ \gamma (3^m + h_3(l) - 2^m - l)
\end{align}
</div>


<p>The $\gamma$ term is really $h_3(n) - n$.</p>

<p>The $\beta_0$ term is the same as $h_3(2^m-1-l)$, as can be seen by
observing that in base $3$, $3^m$ is $1$ followed by $m$ zeroes, so
$3^m-1$ is $m$ twos, and $\frac{3^m-1}{2}$ is $m$ ones, in other words
the same representation as the binary representation of $2^m-1$.</p>

<p>Now, the binary representation of $l$ is the same as the
representation in base $3$ of $h_3(l)$ (by definition of $h_3$), so
the binary representation of $2^m-1-l$ is the same as the
representation in base $3$ of $\frac{3^m-1}{2} - h_3(l)$.</p>

<p>With these two observations, it is possible to rewrite $g$ as</p>

<div markdown="0">
\begin{align}
g(1b_m\cdots b_0) &amp; = (\alpha\beta_{b_m}\cdots\beta_{b_0})_3 + \gamma ((1b_m\cdots b_0)_3 - (1b_m\cdots b_0)_2)
\end{align}
</div>


<p>which is the book solution.</p>

<h3>Faster solution</h3>

<p>It is enough to solve for
$\alpha, \beta_0, \beta_1 \ne 0, \gamma = 0$,
and to find the parameters for $g(n) = n$. The first gives $A$,
$B_0$ and $B_1$ directly by the generalised radix-based Josephus
solution, and the second one adds a constraint to solve for $C$ as well.</p>

<h3>Wrapping up</h3>

<p>As can be seen above, approaching the problem from both directions
(solving for known functions and solving for known parameters) can
result in time saved, and simplified expression of the solution.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Concrete Mathematics Chapter 1 Homework Exercises Part 2]]></title>
    <link href="http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-chapter-1-homework-exercises-part-2/"/>
    <updated>2012-01-14T12:14:00+09:00</updated>
    <id>http://blog.wakatta.jp/blog/2012/01/14/concrete-mathematics-chapter-1-homework-exercises-part-2</id>
    <content type="html"><![CDATA[<p>I finally finished the homework exercises.</p>

<!-- more -->


<h2>Homework Exercises Part 2</h2>

<h3>Generalized Tower of Hanoi</h3>

<p>To solve this, I first observed that for $n=1$, we need $m_1$ moves,
and for $n \gt 1$, we need
$A(m_1, \cdots, m_{n-1}) + m_n + A(m_1, \cdots, m_{n-1})$ or
$2A(m_1, \cdots, m_{n-1}) + m_n$ moves.</p>

<p>This leads to the solution,</p>

<div markdown="0">
\begin{align}
A(m_1, \cdots, m_n) &amp;= \sum_{i=1}^n m_i 2^{n-i}\\\\
\end{align}
</div>


<p>which is trivially shown by induction. The base case:</p>

<div markdown="0">
\begin{align}
A(m_1) &amp; = \sum_{i=1}^1 m_i 2^{1-i}\\\\
&amp; = m_1 2^0\\\\
&amp; = m_1
\end{align}
</div>


<p>And for larger $n$, assuming
$A(m_1, \cdots, m_n) = \sum_{i=1}^n m_i 2^{n-i}$,</p>

<div markdown="0">
\begin{align}
A(m_1, \cdots, m_{n+1}) &amp; = 2A(m_1, \cdots, m_n) +
m_{n+1}&amp;&amp;\text{by definition}\\\\
&amp; = 2\sum_{i=1}^n m_i 2^{n-i} + m_{n+1}&amp;&amp;\text{induction hypothesis}\\\\
&amp; = \sum_{i=1}^{n} m_i 2^{n+1-i} + m_{n+1} 2^{0}\\\\
&amp; = \sum_{i=1}^{n+1} m_i 2^{n+1-i}\\\\
\end{align}
</div>


<h3>Zig-zag lines</h3>

<p>A geometric problem, but very similar to the previous intersecting
lines. A zig-zag is made of 3 segments, so a pair of zig-zag lines can
intersect at 9 different points. The first zig-zag line defines two
regions; each new zig-zag adds a new region, plus one more for each
intersection point.</p>

<p>This gives the following recurrence equations:</p>

<div markdown="0">
\begin{align}
ZZ_1 &amp; = 2\\\\
ZZ_n &amp; = ZZ_{n-1} + 9(n-1) + 1\\\\
\end{align}
</div>


<p>Using the linearity of the recurrence equation, it is easy to see that</p>

<div markdown="0">
\begin{align}
ZZ_n &amp; = ZZ_1 + 9S_{n-1} + (n-1)
\end{align}
</div>


<p>Here I used the linearity to compute solutions to both
$ZZ_n = ZZ_{n-1} + 9(n-1)$ and $ZZ_n = ZZ_{n-1} + 1$, which are
equally trivial. Then I combined the solutions into one.</p>

<p>I use (again) induction to confirm the solution. The base case is
$ZZ_1 = ZZ_1 + 9S_0 + 0$. And for other $n$, assuming
$ZZ_n = ZZ_1 + 9S_{n-1} + (n-1)$</p>

<div markdown="0">
\begin{align}
ZZ_{n+1} &amp; = ZZ_{n} + 9n + 1&amp;&amp;\text{by definition}\\\\
&amp; = ZZ_1 + 9S_{n-1} + (n-1) + 9n + 1&amp;&amp;\text{induction hypothesis}\\\\
&amp; = ZZ_1 + 9(S_{n-1} + n) + (n-1+1)\\\\
&amp; = ZZ_1 + 9S_n + n
\end{align}
</div>


<p>The formula can also be written as</p>

<div markdown="0">
\begin{align}
ZZ_n &amp; = \frac{9n^2-7n+2}{2}
\end{align}
</div>


<h3>Planes cutting cheese</h3>

<p>Again, a geometric problem. This one gave me more trouble. It
took me a while before finally seeing that a new plane intersection
with the previous ones will be a set of intersecting lines which
defines the regions the new plan will divide in two.</p>

<p>The number of regions formed by intersecting lines was solved in the
book, and defined as $L_n = S_n + 1$</p>

<p>So a plane cutting $n$ existing planes will define
$P_{n+1} = P_n + L_n$
new regions. This recurrence gives $P_5 = 26$ regions.</p>

<p>The book did not expect a closed formula for this exercise, as the
necessary techniques are only covered in chapter 5.</p>

<h3>Josephus co-conspirator</h3>

<p>The recurrence equation for $I(n)$ follow the structure of $J(n)$, but
with different base cases:</p>

<div markdown="0">
\begin{align}
I(2) &amp; = 2&amp;&amp;\text{\(I(1)\) is not defined}\\\\
I(2n) &amp; = 2I(n) - 1\\\\
I(2n+1) &amp; = 2I(n) + 1
\end{align}
</div>


<p>Here I generated the first few values to get inspired. I noticed that
$I(n)$ had increasing odd values for batches that were longer than for
$J(n)$: $3, 6, 12, 24, \cdots$.</p>

<p>These numbers are from the series $3\cdot 2^m$, so using the same
"intuitive" step as in the book, I tried to show that
$I(3\cdot 2^m + l) = 2l + 1$ with $0 \le l \lt 3\cdot 2^m$
(the formula does not work for $I(2)$, which has to be defined separately).</p>

<p>By induction on $m$: the base case is $I(3) = I(3\cdot 2^0 + l) = 1$.</p>

<p>Assuming $I(3\cdot 2^m + l) = 2l+1$, we have</p>

<div markdown="0">
\begin{align}
I(3\cdot2^{m+1} + 2l) &amp; = 2I(3\cdot 2^m + l) -1&amp;&amp;\text{by definition}\\\\
&amp;= 2(2l+1) -1&amp;&amp;\text{induction hypothesis}\\\\
&amp;= 4l+2-1\\\\
&amp;= 2(2l)+1\\\\
I(3\cdot 2^{m+1} + (2l+ 1)) &amp; = 2I(3\cdot 2^m + l) + 1&amp;&amp;\text{by definition}\\\\
&amp; = 2(2l+1) + 1&amp;&amp;\text{induction hypothesis}\\\\
\end{align}
</div>


<p>The book solution is defined in terms of $2^m+2^{m-1}+k$, which is
same:</p>

<div markdown="0">
\begin{align}
2^m+2^{m-1}+k &amp; = 2\cdot 2^{m-1} + 2^{m-1} + k\\\\
&amp; = 3\cdot 2^{m-1} + k
\end{align}
</div>


<p>with $1 \le m$, while I have $0 \le m$.</p>

<h3>Repertoire method</h3>

<p>I put the repertoire method in its own
<a href="/blog/2012/01/14/concrete-mathematics-repertoire-method/">post</a> as it
was both the most difficult exercise and the one where I learned the most.</p>
]]></content>
  </entry>
  
</feed>
